{"cells":[{"cell_type":"markdown","metadata":{"id":"GzTwa1dklgs5"},"source":["# Chapter 5.1 - Text Classification with Logistic Regression"]},{"cell_type":"markdown","metadata":{},"source":["Our first machine-learning challenge is to create a prediction model that categorizes news articles with the appropriate categories from a set of 31 categories: politics, entertainment, etc. The model uses logistic regression techniques in Python on a dataset with headlines, short descriptions, and URLs. \n","\n","*NOTE*: Be sure to watch and read the materials posted in the Canvas module before and while you work through this notebook.\n","\n","**Learning Objectives**\n","\n","1. Import and run EDA techniques to understand the potential limits and affordances of the dataset with our ML goal in mind.\n","2. Learn about the basic mechanics of logistic regression (LR).\n","3. Apply LR to this text classification goal of categorizing the news genre of articles based on potential \"features\" in the data, such as the article's headline, short description, and URL."]},{"cell_type":"markdown","metadata":{},"source":["**Sources**\n","\n","- Notebook modified from Ganesan's LR example exercise: [Text Classification with Logistic Regression](https://github.com/kavgan/nlp-in-practice/tree/master/text-classification)\n","- Dataset: HuffPost News Dataset in [../data/05-ML](../data/05-ML/news_category_dataset.json)"]},{"cell_type":"markdown","metadata":{"id":"qJnzMoFVlgs8"},"source":["## Import Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# You may need to install some of the libraries below\n","# If so, uncomment any of the below commands\n","# !pip install matplotlib\n","# !pip install seaborn\n","# !pip install mplcyberpunk\n","# !pip install scikit-learn\n","# !pip install pickle"]},{"cell_type":"code","execution_count":543,"metadata":{"executionInfo":{"elapsed":149,"status":"ok","timestamp":1649708820376,"user":{"displayName":"Chris Lindgren","userId":"04896671906605823933"},"user_tz":240},"id":"q3fZFMM_lgs9"},"outputs":[],"source":["import pandas as pd\n","import regex as re\n","import numpy as np\n","import logging\n","\n","# Data visualization\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import mplcyberpunk\n","%config InlineBackend.figure_formats = ['svg']\n","\n","# ML Modeling\n","from sklearn.metrics import precision_recall_fscore_support,confusion_matrix\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n","from sklearn.preprocessing import LabelBinarizer\n","from sklearn.metrics import RocCurveDisplay,roc_curve\n","from sklearn.preprocessing import normalize\n","# Saving and importing trained models\n","import pickle"]},{"cell_type":"markdown","metadata":{"id":"8SmcYeEklgtB"},"source":["## 1. Refresher on pandas' dataframes"]},{"cell_type":"markdown","metadata":{},"source":["We can use the pandas library to read in, review, and revise the data set.\n","\n","Recall how pandas' \"panel data\" helps us more easily transform multiple types of structured data into what's called a *DataFrame*. Dataframe's are two-dimensional tabular data that are mutable (transformable) with labeled axes (rows and columns). See chapter 3 to refresh your memory, if needed. You can also reference [pandas in general](https://pandas.pydata.org/docs/getting_started/index.html#getting-started), or its [Dataframe datatype](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html#pandas.DataFrame).\n","\n","<figure>\n","<img src=\"../images/01_table_dataframe.svg\"\n","     style=\"max-width: 300px; background: white;\"\n","/>\n","<figcaption>Panel data \"Dataframe\" that shows how there are rows for observations and columns for properties of those observations, as well as an index column for easy reference per row.\n","</figure>"]},{"cell_type":"markdown","metadata":{},"source":["## 2. Import and Review the Data"]},{"cell_type":"code","execution_count":258,"metadata":{"executionInfo":{"elapsed":991,"status":"ok","timestamp":1649709032342,"user":{"displayName":"Chris Lindgren","userId":"04896671906605823933"},"user_tz":240},"id":"wK1wBFMFlgtC"},"outputs":[],"source":[" \n","# Imports the noted JSON file as a pandas DataFrame based on the path below\n","df = pd.read_json(\"./../data/05-ML/news_category_dataset.json\", lines=True)"]},{"cell_type":"markdown","metadata":{},"source":["### 2.1 General shape and content of the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.shape"]},{"cell_type":"markdown","metadata":{},"source":["`.shape` returns a tuple with the panel data 2-dimensional info: 1. the number of rows, and 2. the number of columns."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1649709032518,"user":{"displayName":"Chris Lindgren","userId":"04896671906605823933"},"user_tz":240},"id":"7zw_3EQPlgtC","outputId":"63a502ab-5e06-4d8d-c230-3578444ff853"},"outputs":[],"source":["df.info()"]},{"cell_type":"markdown","metadata":{},"source":["Looks like there are no `null` values in the dataset.\n","\n","Let's dig deeper into the columns and values."]},{"cell_type":"markdown","metadata":{"id":"B6USsXeFlgtF"},"source":["### 2.2 Review particular columns"]},{"cell_type":"markdown","metadata":{},"source":["Be sure to review the data per column. The goals of this modeling may be set for you, but you will be asked to perform similar EDA work on your final project, so you can better understand the modeling possibilities and boundaries with your data. So, the below includes a series of exercises for you to understand this dataset before you conduct the actual modeling work."]},{"cell_type":"code","execution_count":259,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1649709032655,"user":{"displayName":"Chris Lindgren","userId":"04896671906605823933"},"user_tz":240},"id":"3dzu8AdylgtF","outputId":"60bad5c1-1fb4-4e4f-f679-e4d4c1493f07"},"outputs":[{"data":{"text/plain":["Index(['short_description', 'headline', 'date', 'link', 'authors', 'category'], dtype='object')"]},"execution_count":259,"metadata":{},"output_type":"execute_result"}],"source":["# List the columns for reference\n","df.columns"]},{"cell_type":"markdown","metadata":{},"source":["#### 2.2.1 Describe and examine the `headline` column"]},{"cell_type":"markdown","metadata":{},"source":["We can use `.describe()` on a Series, so we can consider any potential quirks. \n","\n","Let's check out the `headline` column, since that's an important column for training our model."]},{"cell_type":"markdown","metadata":{},"source":["##### What are the summary stats for the `headline`?"]},{"cell_type":"code","execution_count":260,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":389},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1649709032519,"user":{"displayName":"Chris Lindgren","userId":"04896671906605823933"},"user_tz":240},"id":"16vrEVmAlgtD","outputId":"f9d9c0dc-5a57-4521-b2d1-dacb03c32ae7"},"outputs":[{"data":{"text/plain":["count             124989\n","unique            124560\n","top       Sunday Roundup\n","freq                  90\n","Name: headline, dtype: object"]},"execution_count":260,"metadata":{},"output_type":"execute_result"}],"source":["df.headline.describe()"]},{"cell_type":"markdown","metadata":{},"source":["Some initial observations: \n","\n","- Noticing how there is a decently sized difference between the `count` and `unique` values\n","- `top`: How \"Sunday Roundup\" headline appears 90 times (`freq`) in the `headline` column (`Name: headline, dtype: object`).\n","\n","Let's review rows with the \"Sunday Roundup\" value in the headline column by using our new skills with pandas. Below I ...\n","\n","1. `df.loc[]`: query the dataframe with the location method\n","2. `[df.headline]`: Specifiy what slice of the data I want to isolate.\n","3. `.str.contains('Sunday Roundup')`: Since `headline` values are Strings, and I want to isolate any rows with the \"Sunday Roundup\" headline, search the isolated Series, `headline`,  with the `.str.contains()` method. It takes a string as its main parameter."]},{"cell_type":"code","execution_count":268,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>short_description</th>\n","      <th>headline</th>\n","      <th>date</th>\n","      <th>link</th>\n","      <th>authors</th>\n","      <th>category</th>\n","      <th>headline_length</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>104119</th>\n","      <td>This week, the Senate released its report on A...</td>\n","      <td>Sunday Roundup</td>\n","      <td>2014-12-14</td>\n","      <td>https://www.huffingtonpost.com/entry/sunday-ro...</td>\n","      <td>Arianna Huffington, Contributor</td>\n","      <td>POLITICS</td>\n","      <td>14</td>\n","    </tr>\n","    <tr>\n","      <th>68558</th>\n","      <td>DAVOS, SWITZERLAND -- This week, while candida...</td>\n","      <td>Sunday Roundup</td>\n","      <td>2016-01-24</td>\n","      <td>https://www.huffingtonpost.com/entry/sunday-ro...</td>\n","      <td>Arianna Huffington, Contributor</td>\n","      <td>POLITICS</td>\n","      <td>14</td>\n","    </tr>\n","    <tr>\n","      <th>102915</th>\n","      <td>This week, a world facing crises on many front...</td>\n","      <td>Sunday Roundup</td>\n","      <td>2014-12-28</td>\n","      <td>https://www.huffingtonpost.com/entry/sunday-ro...</td>\n","      <td>Arianna Huffington, Contributor</td>\n","      <td>POLITICS</td>\n","      <td>14</td>\n","    </tr>\n","    <tr>\n","      <th>113836</th>\n","      <td>This week was dominated by news from Ferguson ...</td>\n","      <td>Sunday Roundup</td>\n","      <td>2014-08-24</td>\n","      <td>https://www.huffingtonpost.com/entry/sunday-ro...</td>\n","      <td>Arianna Huffington, Contributor</td>\n","      <td>POLITICS</td>\n","      <td>14</td>\n","    </tr>\n","    <tr>\n","      <th>116339</th>\n","      <td>This week provided some notable examples of Cr...</td>\n","      <td>Sunday Roundup</td>\n","      <td>2014-07-27</td>\n","      <td>https://www.huffingtonpost.com/entry/sunday-ro...</td>\n","      <td>Arianna Huffington, Contributor</td>\n","      <td>POLITICS</td>\n","      <td>14</td>\n","    </tr>\n","    <tr>\n","      <th>115701</th>\n","      <td>This week Congress lived down to the standards...</td>\n","      <td>Sunday Roundup</td>\n","      <td>2014-08-03</td>\n","      <td>https://www.huffingtonpost.com/entry/sunday-ro...</td>\n","      <td>Arianna Huffington, Contributor</td>\n","      <td>POLITICS</td>\n","      <td>14</td>\n","    </tr>\n","    <tr>\n","      <th>115097</th>\n","      <td>This week we went back to the future as Presid...</td>\n","      <td>Sunday Roundup</td>\n","      <td>2014-08-10</td>\n","      <td>https://www.huffingtonpost.com/entry/sunday-ro...</td>\n","      <td>Arianna Huffington, Contributor</td>\n","      <td>POLITICS</td>\n","      <td>14</td>\n","    </tr>\n","    <tr>\n","      <th>117597</th>\n","      <td>This week, the spotlight remained on immigrati...</td>\n","      <td>Sunday Roundup</td>\n","      <td>2014-07-13</td>\n","      <td>https://www.huffingtonpost.com/entry/sunday-ro...</td>\n","      <td>Arianna Huffington, Contributor</td>\n","      <td>POLITICS</td>\n","      <td>14</td>\n","    </tr>\n","    <tr>\n","      <th>120664</th>\n","      <td>With Friday's numbers showing the addition of ...</td>\n","      <td>Sunday Roundup</td>\n","      <td>2014-06-08</td>\n","      <td>https://www.huffingtonpost.com/entry/sunday-ro...</td>\n","      <td>Arianna Huffington, Contributor</td>\n","      <td>POLITICS</td>\n","      <td>14</td>\n","    </tr>\n","    <tr>\n","      <th>81567</th>\n","      <td>This week brought powerful reminders of what h...</td>\n","      <td>Sunday Roundup</td>\n","      <td>2015-08-30</td>\n","      <td>https://www.huffingtonpost.com/entry/sunday-ro...</td>\n","      <td>Arianna Huffington, Contributor</td>\n","      <td>POLITICS</td>\n","      <td>14</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                        short_description        headline  \\\n","104119  This week, the Senate released its report on A...  Sunday Roundup   \n","68558   DAVOS, SWITZERLAND -- This week, while candida...  Sunday Roundup   \n","102915  This week, a world facing crises on many front...  Sunday Roundup   \n","113836  This week was dominated by news from Ferguson ...  Sunday Roundup   \n","116339  This week provided some notable examples of Cr...  Sunday Roundup   \n","115701  This week Congress lived down to the standards...  Sunday Roundup   \n","115097  This week we went back to the future as Presid...  Sunday Roundup   \n","117597  This week, the spotlight remained on immigrati...  Sunday Roundup   \n","120664  With Friday's numbers showing the addition of ...  Sunday Roundup   \n","81567   This week brought powerful reminders of what h...  Sunday Roundup   \n","\n","             date                                               link  \\\n","104119 2014-12-14  https://www.huffingtonpost.com/entry/sunday-ro...   \n","68558  2016-01-24  https://www.huffingtonpost.com/entry/sunday-ro...   \n","102915 2014-12-28  https://www.huffingtonpost.com/entry/sunday-ro...   \n","113836 2014-08-24  https://www.huffingtonpost.com/entry/sunday-ro...   \n","116339 2014-07-27  https://www.huffingtonpost.com/entry/sunday-ro...   \n","115701 2014-08-03  https://www.huffingtonpost.com/entry/sunday-ro...   \n","115097 2014-08-10  https://www.huffingtonpost.com/entry/sunday-ro...   \n","117597 2014-07-13  https://www.huffingtonpost.com/entry/sunday-ro...   \n","120664 2014-06-08  https://www.huffingtonpost.com/entry/sunday-ro...   \n","81567  2015-08-30  https://www.huffingtonpost.com/entry/sunday-ro...   \n","\n","                                authors  category  headline_length  \n","104119  Arianna Huffington, Contributor  POLITICS               14  \n","68558   Arianna Huffington, Contributor  POLITICS               14  \n","102915  Arianna Huffington, Contributor  POLITICS               14  \n","113836  Arianna Huffington, Contributor  POLITICS               14  \n","116339  Arianna Huffington, Contributor  POLITICS               14  \n","115701  Arianna Huffington, Contributor  POLITICS               14  \n","115097  Arianna Huffington, Contributor  POLITICS               14  \n","117597  Arianna Huffington, Contributor  POLITICS               14  \n","120664  Arianna Huffington, Contributor  POLITICS               14  \n","81567   Arianna Huffington, Contributor  POLITICS               14  "]},"execution_count":268,"metadata":{},"output_type":"execute_result"}],"source":["df.loc[df.headline.str.contains('Sunday Roundup')].sample(10)"]},{"cell_type":"markdown","metadata":{},"source":["##### How long are the headlines? (What's the distribution?)\n","\n","Also curious about the distribution of the length of those headlines, so let's add a new column to the `df` by applying the `len` (length) method to the `headline` column. Below, I do so with the `apply()` method and assign the values per row to a new Series (column) that I call `headline_length`."]},{"cell_type":"code","execution_count":269,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>short_description</th>\n","      <th>headline</th>\n","      <th>date</th>\n","      <th>link</th>\n","      <th>authors</th>\n","      <th>category</th>\n","      <th>headline_length</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>She left her husband. He killed their children...</td>\n","      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n","      <td>2018-05-26</td>\n","      <td>https://www.huffingtonpost.com/entry/texas-ama...</td>\n","      <td>Melissa Jeltsen</td>\n","      <td>CRIME</td>\n","      <td>64</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Of course it has a song.</td>\n","      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n","      <td>2018-05-26</td>\n","      <td>https://www.huffingtonpost.com/entry/will-smit...</td>\n","      <td>Andy McDonald</td>\n","      <td>ENTERTAINMENT</td>\n","      <td>75</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>The actor and his longtime girlfriend Anna Ebe...</td>\n","      <td>Hugh Grant Marries For The First Time At Age 57</td>\n","      <td>2018-05-26</td>\n","      <td>https://www.huffingtonpost.com/entry/hugh-gran...</td>\n","      <td>Ron Dicker</td>\n","      <td>ENTERTAINMENT</td>\n","      <td>47</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>The actor gives Dems an ass-kicking for not fi...</td>\n","      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n","      <td>2018-05-26</td>\n","      <td>https://www.huffingtonpost.com/entry/jim-carre...</td>\n","      <td>Ron Dicker</td>\n","      <td>ENTERTAINMENT</td>\n","      <td>69</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>The \"Dietland\" actress said using the bags is ...</td>\n","      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n","      <td>2018-05-26</td>\n","      <td>https://www.huffingtonpost.com/entry/julianna-...</td>\n","      <td>Ron Dicker</td>\n","      <td>ENTERTAINMENT</td>\n","      <td>71</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                   short_description  \\\n","0  She left her husband. He killed their children...   \n","1                           Of course it has a song.   \n","2  The actor and his longtime girlfriend Anna Ebe...   \n","3  The actor gives Dems an ass-kicking for not fi...   \n","4  The \"Dietland\" actress said using the bags is ...   \n","\n","                                            headline       date  \\\n","0  There Were 2 Mass Shootings In Texas Last Week... 2018-05-26   \n","1  Will Smith Joins Diplo And Nicky Jam For The 2... 2018-05-26   \n","2    Hugh Grant Marries For The First Time At Age 57 2018-05-26   \n","3  Jim Carrey Blasts 'Castrato' Adam Schiff And D... 2018-05-26   \n","4  Julianna Margulies Uses Donald Trump Poop Bags... 2018-05-26   \n","\n","                                                link          authors  \\\n","0  https://www.huffingtonpost.com/entry/texas-ama...  Melissa Jeltsen   \n","1  https://www.huffingtonpost.com/entry/will-smit...    Andy McDonald   \n","2  https://www.huffingtonpost.com/entry/hugh-gran...       Ron Dicker   \n","3  https://www.huffingtonpost.com/entry/jim-carre...       Ron Dicker   \n","4  https://www.huffingtonpost.com/entry/julianna-...       Ron Dicker   \n","\n","        category  headline_length  \n","0          CRIME               64  \n","1  ENTERTAINMENT               75  \n","2  ENTERTAINMENT               47  \n","3  ENTERTAINMENT               69  \n","4  ENTERTAINMENT               71  "]},"execution_count":269,"metadata":{},"output_type":"execute_result"}],"source":["df['headline_length'] = df.headline.apply(len)\n","df.head()"]},{"cell_type":"code","execution_count":270,"metadata":{},"outputs":[{"data":{"text/plain":["<AxesSubplot: >"]},"execution_count":270,"metadata":{},"output_type":"execute_result"},{"data":{"image/svg+xml":["<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n","<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n","  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n","<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"715.304688pt\" height=\"359.685313pt\" viewBox=\"0 0 715.304688 359.685313\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n"," <metadata>\n","  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n","   <cc:Work>\n","    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n","    <dc:date>2023-11-03T17:13:35.343733</dc:date>\n","    <dc:format>image/svg+xml</dc:format>\n","    <dc:creator>\n","     <cc:Agent>\n","      <dc:title>Matplotlib v3.6.0, https://matplotlib.org/</dc:title>\n","     </cc:Agent>\n","    </dc:creator>\n","   </cc:Work>\n","  </rdf:RDF>\n"," </metadata>\n"," <defs>\n","  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n"," </defs>\n"," <g id=\"figure_1\">\n","  <g id=\"patch_1\">\n","   <path d=\"M 0 359.685313 \n","L 715.304688 359.685313 \n","L 715.304688 0 \n","L 0 0 \n","z\n","\" style=\"fill: #212946\"/>\n","  </g>\n","  <g id=\"axes_1\">\n","   <g id=\"patch_2\">\n","    <path d=\"M 38.504687 339.84 \n","L 708.104687 339.84 \n","L 708.104687 7.2 \n","L 38.504687 7.2 \n","z\n","\" style=\"fill: #212946\"/>\n","   </g>\n","   <g id=\"matplotlib.axis_1\">\n","    <g id=\"xtick_1\">\n","     <g id=\"line2d_1\">\n","      <path d=\"M 68.941051 339.84 \n","L 68.941051 7.2 \n","\" clip-path=\"url(#p9c87f682a8)\" style=\"fill: none; stroke: #2a3459; stroke-width: 0.8; stroke-linecap: round\"/>\n","     </g>\n","     <g id=\"line2d_2\"/>\n","     <g id=\"text_1\">\n","      <!-- 0 -->\n","      <g style=\"fill: #e6e6e6\" transform=\"translate(66.160582 350.497813) scale(0.1 -0.1)\">\n","       <defs>\n","        <path id=\"ArialMT-30\" d=\"M 266 2259 \n","Q 266 3072 433 3567 \n","Q 600 4063 929 4331 \n","Q 1259 4600 1759 4600 \n","Q 2128 4600 2406 4451 \n","Q 2684 4303 2865 4023 \n","Q 3047 3744 3150 3342 \n","Q 3253 2941 3253 2259 \n","Q 3253 1453 3087 958 \n","Q 2922 463 2592 192 \n","Q 2263 -78 1759 -78 \n","Q 1097 -78 719 397 \n","Q 266 969 266 2259 \n","z\n","M 844 2259 \n","Q 844 1131 1108 757 \n","Q 1372 384 1759 384 \n","Q 2147 384 2411 759 \n","Q 2675 1134 2675 2259 \n","Q 2675 3391 2411 3762 \n","Q 2147 4134 1753 4134 \n","Q 1366 4134 1134 3806 \n","Q 844 3388 844 2259 \n","z\n","\" transform=\"scale(0.015625)\"/>\n","       </defs>\n","       <use xlink:href=\"#ArialMT-30\"/>\n","      </g>\n","     </g>\n","    </g>\n","    <g id=\"xtick_2\">\n","     <g id=\"line2d_3\">\n","      <path d=\"M 164.054688 339.84 \n","L 164.054688 7.2 \n","\" clip-path=\"url(#p9c87f682a8)\" style=\"fill: none; stroke: #2a3459; stroke-width: 0.8; stroke-linecap: round\"/>\n","     </g>\n","     <g id=\"line2d_4\"/>\n","     <g id=\"text_2\">\n","      <!-- 50 -->\n","      <g style=\"fill: #e6e6e6\" transform=\"translate(158.49375 350.497813) scale(0.1 -0.1)\">\n","       <defs>\n","        <path id=\"ArialMT-35\" d=\"M 266 1200 \n","L 856 1250 \n","Q 922 819 1161 601 \n","Q 1400 384 1738 384 \n","Q 2144 384 2425 690 \n","Q 2706 997 2706 1503 \n","Q 2706 1984 2436 2262 \n","Q 2166 2541 1728 2541 \n","Q 1456 2541 1237 2417 \n","Q 1019 2294 894 2097 \n","L 366 2166 \n","L 809 4519 \n","L 3088 4519 \n","L 3088 3981 \n","L 1259 3981 \n","L 1013 2750 \n","Q 1425 3038 1878 3038 \n","Q 2478 3038 2890 2622 \n","Q 3303 2206 3303 1553 \n","Q 3303 931 2941 478 \n","Q 2500 -78 1738 -78 \n","Q 1113 -78 717 272 \n","Q 322 622 266 1200 \n","z\n","\" transform=\"scale(0.015625)\"/>\n","       </defs>\n","       <use xlink:href=\"#ArialMT-35\"/>\n","       <use xlink:href=\"#ArialMT-30\" x=\"55.615234\"/>\n","      </g>\n","     </g>\n","    </g>\n","    <g id=\"xtick_3\">\n","     <g id=\"line2d_5\">\n","      <path d=\"M 259.168324 339.84 \n","L 259.168324 7.2 \n","\" clip-path=\"url(#p9c87f682a8)\" style=\"fill: none; stroke: #2a3459; stroke-width: 0.8; stroke-linecap: round\"/>\n","     </g>\n","     <g id=\"line2d_6\"/>\n","     <g id=\"text_3\">\n","      <!-- 100 -->\n","      <g style=\"fill: #e6e6e6\" transform=\"translate(250.826918 350.497813) scale(0.1 -0.1)\">\n","       <defs>\n","        <path id=\"ArialMT-31\" d=\"M 2384 0 \n","L 1822 0 \n","L 1822 3584 \n","Q 1619 3391 1289 3197 \n","Q 959 3003 697 2906 \n","L 697 3450 \n","Q 1169 3672 1522 3987 \n","Q 1875 4303 2022 4600 \n","L 2384 4600 \n","L 2384 0 \n","z\n","\" transform=\"scale(0.015625)\"/>\n","       </defs>\n","       <use xlink:href=\"#ArialMT-31\"/>\n","       <use xlink:href=\"#ArialMT-30\" x=\"55.615234\"/>\n","       <use xlink:href=\"#ArialMT-30\" x=\"111.230469\"/>\n","      </g>\n","     </g>\n","    </g>\n","    <g id=\"xtick_4\">\n","     <g id=\"line2d_7\">\n","      <path d=\"M 354.28196 339.84 \n","L 354.28196 7.2 \n","\" clip-path=\"url(#p9c87f682a8)\" style=\"fill: none; stroke: #2a3459; stroke-width: 0.8; stroke-linecap: round\"/>\n","     </g>\n","     <g id=\"line2d_8\"/>\n","     <g id=\"text_4\">\n","      <!-- 150 -->\n","      <g style=\"fill: #e6e6e6\" transform=\"translate(345.940554 350.497813) scale(0.1 -0.1)\">\n","       <use xlink:href=\"#ArialMT-31\"/>\n","       <use xlink:href=\"#ArialMT-35\" x=\"55.615234\"/>\n","       <use xlink:href=\"#ArialMT-30\" x=\"111.230469\"/>\n","      </g>\n","     </g>\n","    </g>\n","    <g id=\"xtick_5\">\n","     <g id=\"line2d_9\">\n","      <path d=\"M 449.395597 339.84 \n","L 449.395597 7.2 \n","\" clip-path=\"url(#p9c87f682a8)\" style=\"fill: none; stroke: #2a3459; stroke-width: 0.8; stroke-linecap: round\"/>\n","     </g>\n","     <g id=\"line2d_10\"/>\n","     <g id=\"text_5\">\n","      <!-- 200 -->\n","      <g style=\"fill: #e6e6e6\" transform=\"translate(441.05419 350.497813) scale(0.1 -0.1)\">\n","       <defs>\n","        <path id=\"ArialMT-32\" d=\"M 3222 541 \n","L 3222 0 \n","L 194 0 \n","Q 188 203 259 391 \n","Q 375 700 629 1000 \n","Q 884 1300 1366 1694 \n","Q 2113 2306 2375 2664 \n","Q 2638 3022 2638 3341 \n","Q 2638 3675 2398 3904 \n","Q 2159 4134 1775 4134 \n","Q 1369 4134 1125 3890 \n","Q 881 3647 878 3216 \n","L 300 3275 \n","Q 359 3922 746 4261 \n","Q 1134 4600 1788 4600 \n","Q 2447 4600 2831 4234 \n","Q 3216 3869 3216 3328 \n","Q 3216 3053 3103 2787 \n","Q 2991 2522 2730 2228 \n","Q 2469 1934 1863 1422 \n","Q 1356 997 1212 845 \n","Q 1069 694 975 541 \n","L 3222 541 \n","z\n","\" transform=\"scale(0.015625)\"/>\n","       </defs>\n","       <use xlink:href=\"#ArialMT-32\"/>\n","       <use xlink:href=\"#ArialMT-30\" x=\"55.615234\"/>\n","       <use xlink:href=\"#ArialMT-30\" x=\"111.230469\"/>\n","      </g>\n","     </g>\n","    </g>\n","    <g id=\"xtick_6\">\n","     <g id=\"line2d_11\">\n","      <path d=\"M 544.509233 339.84 \n","L 544.509233 7.2 \n","\" clip-path=\"url(#p9c87f682a8)\" style=\"fill: none; stroke: #2a3459; stroke-width: 0.8; stroke-linecap: round\"/>\n","     </g>\n","     <g id=\"line2d_12\"/>\n","     <g id=\"text_6\">\n","      <!-- 250 -->\n","      <g style=\"fill: #e6e6e6\" transform=\"translate(536.167827 350.497813) scale(0.1 -0.1)\">\n","       <use xlink:href=\"#ArialMT-32\"/>\n","       <use xlink:href=\"#ArialMT-35\" x=\"55.615234\"/>\n","       <use xlink:href=\"#ArialMT-30\" x=\"111.230469\"/>\n","      </g>\n","     </g>\n","    </g>\n","    <g id=\"xtick_7\">\n","     <g id=\"line2d_13\">\n","      <path d=\"M 639.622869 339.84 \n","L 639.622869 7.2 \n","\" clip-path=\"url(#p9c87f682a8)\" style=\"fill: none; stroke: #2a3459; stroke-width: 0.8; stroke-linecap: round\"/>\n","     </g>\n","     <g id=\"line2d_14\"/>\n","     <g id=\"text_7\">\n","      <!-- 300 -->\n","      <g style=\"fill: #e6e6e6\" transform=\"translate(631.281463 350.497813) scale(0.1 -0.1)\">\n","       <defs>\n","        <path id=\"ArialMT-33\" d=\"M 269 1209 \n","L 831 1284 \n","Q 928 806 1161 595 \n","Q 1394 384 1728 384 \n","Q 2125 384 2398 659 \n","Q 2672 934 2672 1341 \n","Q 2672 1728 2419 1979 \n","Q 2166 2231 1775 2231 \n","Q 1616 2231 1378 2169 \n","L 1441 2663 \n","Q 1497 2656 1531 2656 \n","Q 1891 2656 2178 2843 \n","Q 2466 3031 2466 3422 \n","Q 2466 3731 2256 3934 \n","Q 2047 4138 1716 4138 \n","Q 1388 4138 1169 3931 \n","Q 950 3725 888 3313 \n","L 325 3413 \n","Q 428 3978 793 4289 \n","Q 1159 4600 1703 4600 \n","Q 2078 4600 2393 4439 \n","Q 2709 4278 2876 4000 \n","Q 3044 3722 3044 3409 \n","Q 3044 3113 2884 2869 \n","Q 2725 2625 2413 2481 \n","Q 2819 2388 3044 2092 \n","Q 3269 1797 3269 1353 \n","Q 3269 753 2831 336 \n","Q 2394 -81 1725 -81 \n","Q 1122 -81 723 278 \n","Q 325 638 269 1209 \n","z\n","\" transform=\"scale(0.015625)\"/>\n","       </defs>\n","       <use xlink:href=\"#ArialMT-33\"/>\n","       <use xlink:href=\"#ArialMT-30\" x=\"55.615234\"/>\n","       <use xlink:href=\"#ArialMT-30\" x=\"111.230469\"/>\n","      </g>\n","     </g>\n","    </g>\n","   </g>\n","   <g id=\"matplotlib.axis_2\">\n","    <g id=\"ytick_1\">\n","     <g id=\"line2d_15\">\n","      <path d=\"M 38.504687 339.84 \n","L 708.104687 339.84 \n","\" clip-path=\"url(#p9c87f682a8)\" style=\"fill: none; stroke: #2a3459; stroke-width: 0.8; stroke-linecap: round\"/>\n","     </g>\n","     <g id=\"line2d_16\"/>\n","     <g id=\"text_8\">\n","      <!-- 0 -->\n","      <g style=\"fill: #e6e6e6\" transform=\"translate(29.44375 343.418906) scale(0.1 -0.1)\">\n","       <use xlink:href=\"#ArialMT-30\"/>\n","      </g>\n","     </g>\n","    </g>\n","    <g id=\"ytick_2\">\n","     <g id=\"line2d_17\">\n","      <path d=\"M 38.504687 287.660931 \n","L 708.104687 287.660931 \n","\" clip-path=\"url(#p9c87f682a8)\" style=\"fill: none; stroke: #2a3459; stroke-width: 0.8; stroke-linecap: round\"/>\n","     </g>\n","     <g id=\"line2d_18\"/>\n","     <g id=\"text_9\">\n","      <!-- 10000 -->\n","      <g style=\"fill: #e6e6e6\" transform=\"translate(7.2 291.239837) scale(0.1 -0.1)\">\n","       <use xlink:href=\"#ArialMT-31\"/>\n","       <use xlink:href=\"#ArialMT-30\" x=\"55.615234\"/>\n","       <use xlink:href=\"#ArialMT-30\" x=\"111.230469\"/>\n","       <use xlink:href=\"#ArialMT-30\" x=\"166.845703\"/>\n","       <use xlink:href=\"#ArialMT-30\" x=\"222.460938\"/>\n","      </g>\n","     </g>\n","    </g>\n","    <g id=\"ytick_3\">\n","     <g id=\"line2d_19\">\n","      <path d=\"M 38.504687 235.481862 \n","L 708.104687 235.481862 \n","\" clip-path=\"url(#p9c87f682a8)\" style=\"fill: none; stroke: #2a3459; stroke-width: 0.8; stroke-linecap: round\"/>\n","     </g>\n","     <g id=\"line2d_20\"/>\n","     <g id=\"text_10\">\n","      <!-- 20000 -->\n","      <g style=\"fill: #e6e6e6\" transform=\"translate(7.2 239.060768) scale(0.1 -0.1)\">\n","       <use xlink:href=\"#ArialMT-32\"/>\n","       <use xlink:href=\"#ArialMT-30\" x=\"55.615234\"/>\n","       <use xlink:href=\"#ArialMT-30\" x=\"111.230469\"/>\n","       <use xlink:href=\"#ArialMT-30\" x=\"166.845703\"/>\n","       <use xlink:href=\"#ArialMT-30\" x=\"222.460938\"/>\n","      </g>\n","     </g>\n","    </g>\n","    <g id=\"ytick_4\">\n","     <g id=\"line2d_21\">\n","      <path d=\"M 38.504687 183.302793 \n","L 708.104687 183.302793 \n","\" clip-path=\"url(#p9c87f682a8)\" style=\"fill: none; stroke: #2a3459; stroke-width: 0.8; stroke-linecap: round\"/>\n","     </g>\n","     <g id=\"line2d_22\"/>\n","     <g id=\"text_11\">\n","      <!-- 30000 -->\n","      <g style=\"fill: #e6e6e6\" transform=\"translate(7.2 186.881699) scale(0.1 -0.1)\">\n","       <use xlink:href=\"#ArialMT-33\"/>\n","       <use xlink:href=\"#ArialMT-30\" x=\"55.615234\"/>\n","       <use xlink:href=\"#ArialMT-30\" x=\"111.230469\"/>\n","       <use xlink:href=\"#ArialMT-30\" x=\"166.845703\"/>\n","       <use xlink:href=\"#ArialMT-30\" x=\"222.460938\"/>\n","      </g>\n","     </g>\n","    </g>\n","    <g id=\"ytick_5\">\n","     <g id=\"line2d_23\">\n","      <path d=\"M 38.504687 131.123724 \n","L 708.104687 131.123724 \n","\" clip-path=\"url(#p9c87f682a8)\" style=\"fill: none; stroke: #2a3459; stroke-width: 0.8; stroke-linecap: round\"/>\n","     </g>\n","     <g id=\"line2d_24\"/>\n","     <g id=\"text_12\">\n","      <!-- 40000 -->\n","      <g style=\"fill: #e6e6e6\" transform=\"translate(7.2 134.70263) scale(0.1 -0.1)\">\n","       <defs>\n","        <path id=\"ArialMT-34\" d=\"M 2069 0 \n","L 2069 1097 \n","L 81 1097 \n","L 81 1613 \n","L 2172 4581 \n","L 2631 4581 \n","L 2631 1613 \n","L 3250 1613 \n","L 3250 1097 \n","L 2631 1097 \n","L 2631 0 \n","L 2069 0 \n","z\n","M 2069 1613 \n","L 2069 3678 \n","L 634 1613 \n","L 2069 1613 \n","z\n","\" transform=\"scale(0.015625)\"/>\n","       </defs>\n","       <use xlink:href=\"#ArialMT-34\"/>\n","       <use xlink:href=\"#ArialMT-30\" x=\"55.615234\"/>\n","       <use xlink:href=\"#ArialMT-30\" x=\"111.230469\"/>\n","       <use xlink:href=\"#ArialMT-30\" x=\"166.845703\"/>\n","       <use xlink:href=\"#ArialMT-30\" x=\"222.460938\"/>\n","      </g>\n","     </g>\n","    </g>\n","    <g id=\"ytick_6\">\n","     <g id=\"line2d_25\">\n","      <path d=\"M 38.504687 78.944655 \n","L 708.104687 78.944655 \n","\" clip-path=\"url(#p9c87f682a8)\" style=\"fill: none; stroke: #2a3459; stroke-width: 0.8; stroke-linecap: round\"/>\n","     </g>\n","     <g id=\"line2d_26\"/>\n","     <g id=\"text_13\">\n","      <!-- 50000 -->\n","      <g style=\"fill: #e6e6e6\" transform=\"translate(7.2 82.523561) scale(0.1 -0.1)\">\n","       <use xlink:href=\"#ArialMT-35\"/>\n","       <use xlink:href=\"#ArialMT-30\" x=\"55.615234\"/>\n","       <use xlink:href=\"#ArialMT-30\" x=\"111.230469\"/>\n","       <use xlink:href=\"#ArialMT-30\" x=\"166.845703\"/>\n","       <use xlink:href=\"#ArialMT-30\" x=\"222.460938\"/>\n","      </g>\n","     </g>\n","    </g>\n","    <g id=\"ytick_7\">\n","     <g id=\"line2d_27\">\n","      <path d=\"M 38.504687 26.765586 \n","L 708.104687 26.765586 \n","\" clip-path=\"url(#p9c87f682a8)\" style=\"fill: none; stroke: #2a3459; stroke-width: 0.8; stroke-linecap: round\"/>\n","     </g>\n","     <g id=\"line2d_28\"/>\n","     <g id=\"text_14\">\n","      <!-- 60000 -->\n","      <g style=\"fill: #e6e6e6\" transform=\"translate(7.2 30.344492) scale(0.1 -0.1)\">\n","       <defs>\n","        <path id=\"ArialMT-36\" d=\"M 3184 3459 \n","L 2625 3416 \n","Q 2550 3747 2413 3897 \n","Q 2184 4138 1850 4138 \n","Q 1581 4138 1378 3988 \n","Q 1113 3794 959 3422 \n","Q 806 3050 800 2363 \n","Q 1003 2672 1297 2822 \n","Q 1591 2972 1913 2972 \n","Q 2475 2972 2870 2558 \n","Q 3266 2144 3266 1488 \n","Q 3266 1056 3080 686 \n","Q 2894 316 2569 119 \n","Q 2244 -78 1831 -78 \n","Q 1128 -78 684 439 \n","Q 241 956 241 2144 \n","Q 241 3472 731 4075 \n","Q 1159 4600 1884 4600 \n","Q 2425 4600 2770 4297 \n","Q 3116 3994 3184 3459 \n","z\n","M 888 1484 \n","Q 888 1194 1011 928 \n","Q 1134 663 1356 523 \n","Q 1578 384 1822 384 \n","Q 2178 384 2434 671 \n","Q 2691 959 2691 1453 \n","Q 2691 1928 2437 2201 \n","Q 2184 2475 1800 2475 \n","Q 1419 2475 1153 2201 \n","Q 888 1928 888 1484 \n","z\n","\" transform=\"scale(0.015625)\"/>\n","       </defs>\n","       <use xlink:href=\"#ArialMT-36\"/>\n","       <use xlink:href=\"#ArialMT-30\" x=\"55.615234\"/>\n","       <use xlink:href=\"#ArialMT-30\" x=\"111.230469\"/>\n","       <use xlink:href=\"#ArialMT-30\" x=\"166.845703\"/>\n","       <use xlink:href=\"#ArialMT-30\" x=\"222.460938\"/>\n","      </g>\n","     </g>\n","    </g>\n","   </g>\n","   <g id=\"patch_3\">\n","    <path d=\"M 68.941051 339.84 \n","L 129.813778 339.84 \n","L 129.813778 298.639407 \n","L 68.941051 298.639407 \n","z\n","\" clip-path=\"url(#p9c87f682a8)\" style=\"fill: #86bf91\"/>\n","   </g>\n","   <g id=\"patch_4\">\n","    <path d=\"M 129.813778 339.84 \n","L 190.686506 339.84 \n","L 190.686506 23.04 \n","L 129.813778 23.04 \n","z\n","\" clip-path=\"url(#p9c87f682a8)\" style=\"fill: #86bf91\"/>\n","   </g>\n","   <g id=\"patch_5\">\n","    <path d=\"M 190.686506 339.84 \n","L 251.559233 339.84 \n","L 251.559233 54.623991 \n","L 190.686506 54.623991 \n","z\n","\" clip-path=\"url(#p9c87f682a8)\" style=\"fill: #86bf91\"/>\n","   </g>\n","   <g id=\"patch_6\">\n","    <path d=\"M 251.559233 339.84 \n","L 312.43196 339.84 \n","L 312.43196 331.397427 \n","L 251.559233 331.397427 \n","z\n","\" clip-path=\"url(#p9c87f682a8)\" style=\"fill: #86bf91\"/>\n","   </g>\n","   <g id=\"patch_7\">\n","    <path d=\"M 312.43196 339.84 \n","L 373.304688 339.84 \n","L 373.304688 339.380824 \n","L 312.43196 339.380824 \n","z\n","\" clip-path=\"url(#p9c87f682a8)\" style=\"fill: #86bf91\"/>\n","   </g>\n","   <g id=\"patch_8\">\n","    <path d=\"M 373.304687 339.84 \n","L 434.177415 339.84 \n","L 434.177415 339.803475 \n","L 373.304687 339.803475 \n","z\n","\" clip-path=\"url(#p9c87f682a8)\" style=\"fill: #86bf91\"/>\n","   </g>\n","   <g id=\"patch_9\">\n","    <path d=\"M 434.177415 339.84 \n","L 495.050142 339.84 \n","L 495.050142 339.829564 \n","L 434.177415 339.829564 \n","z\n","\" clip-path=\"url(#p9c87f682a8)\" style=\"fill: #86bf91\"/>\n","   </g>\n","   <g id=\"patch_10\">\n","    <path d=\"M 495.050142 339.84 \n","L 555.922869 339.84 \n","L 555.922869 339.834782 \n","L 495.050142 339.834782 \n","z\n","\" clip-path=\"url(#p9c87f682a8)\" style=\"fill: #86bf91\"/>\n","   </g>\n","   <g id=\"patch_11\">\n","    <path d=\"M 555.922869 339.84 \n","L 616.795597 339.84 \n","L 616.795597 339.84 \n","L 555.922869 339.84 \n","z\n","\" clip-path=\"url(#p9c87f682a8)\" style=\"fill: #86bf91\"/>\n","   </g>\n","   <g id=\"patch_12\">\n","    <path d=\"M 616.795597 339.84 \n","L 677.668324 339.84 \n","L 677.668324 339.829564 \n","L 616.795597 339.829564 \n","z\n","\" clip-path=\"url(#p9c87f682a8)\" style=\"fill: #86bf91\"/>\n","   </g>\n","   <g id=\"patch_13\">\n","    <path d=\"M 38.504687 339.84 \n","L 38.504687 7.2 \n","\" style=\"fill: none\"/>\n","   </g>\n","   <g id=\"patch_14\">\n","    <path d=\"M 708.104687 339.84 \n","L 708.104687 7.2 \n","\" style=\"fill: none\"/>\n","   </g>\n","   <g id=\"patch_15\">\n","    <path d=\"M 38.504687 339.84 \n","L 708.104687 339.84 \n","\" style=\"fill: none\"/>\n","   </g>\n","   <g id=\"patch_16\">\n","    <path d=\"M 38.504687 7.2 \n","L 708.104687 7.2 \n","\" style=\"fill: none\"/>\n","   </g>\n","  </g>\n"," </g>\n"," <defs>\n","  <clipPath id=\"p9c87f682a8\">\n","   <rect x=\"38.504687\" y=\"7.2\" width=\"669.6\" height=\"332.64\"/>\n","  </clipPath>\n"," </defs>\n","</svg>\n"],"text/plain":["<Figure size 1200x600 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["'''\n","  .hist() -- Creates a histogram chart with a dataframe's Series \n","  \n","  Histograms place a metric in bins -- dates in this case -- to understand the distribution of the data. In this case, the distribution of the data over time\n","'''\n","df.headline_length.hist(\n","  figsize=(12,6),\n","  color='#86bf91',\n",")"]},{"cell_type":"markdown","metadata":{},"source":["Looks like there's some variance with some headlines that are over 100 characters long and as much as ~300.\n","\n","Let's use `.describe()` on this new column as a table of values to review."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.headline_length.describe()"]},{"cell_type":"markdown","metadata":{},"source":["##### 2.2.1 Exercise -- Observations about the headlines column\n","\n","- ***ENTER YOUR FIRST OBSERVATION HERE***\n","  - Question: How might this impact our model's output?\n","  - ***YOUR RESPONSE HERE***\n","- ***ENTER YOUR SECOND OBSERVATION HERE***\n","  - Question: How might this impact our model's output?\n","  - ***YOUR RESPONSE HERE***\n","- ***ENTER MORE OBSERVATIONS***\n","  - Question: How might this impact our model's output?\n","  - ***YOUR RESPONSE HERE***"]},{"cell_type":"markdown","metadata":{"id":"eSFKju2BlgtG"},"source":["#### 2.2.2 What are the range and distribution of dates?"]},{"cell_type":"markdown","metadata":{},"source":["Since we have a `date` column, and the datatype is in a standardized format, we can quickly plot the date range in a histogram figure.\n","\n","Let's jot down some observations in our notebook.\n","\n","**NOTE**: Be sure to respond to the questions and add at least one more potential question, observation, and potential explanation for it"]},{"cell_type":"markdown","metadata":{},"source":["##### 2.2.2.1 *EXERCISE* -- Notes on the distribution of the data based on the `date` column"]},{"cell_type":"markdown","metadata":{},"source":["- Articles' publishing dates range between July 2014 and July 2018\n","  - Question: How might this impact the model's output?\n","  - ***YOUR RESPONSE HERE***\n","- Fewer 2018 articles compared to the rest of the dataset.\n","  - Question: How might this impact the model's output?\n","  - ***YOUR RESPONSE HERE***"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":395},"executionInfo":{"elapsed":369,"status":"ok","timestamp":1649709033022,"user":{"displayName":"Chris Lindgren","userId":"04896671906605823933"},"user_tz":240},"id":"7MlAJyRblgtG","outputId":"a4b4d7b1-d885-423e-8135-fbd69e10f7c5"},"outputs":[],"source":["'''\n","  .hist() -- Creates a histogram chart with a dataframe's Series \n","  \n","  Histograms place a metric in bins -- dates in this case -- to understand the distribution of the data. In this case, the distribution of the data over time\n","'''\n","\n","df.date.hist(\n","  figsize=(12,6),\n","  color='#86bf91',\n",")"]},{"cell_type":"markdown","metadata":{},"source":["#### 2.2.3 Describe and examine the `category` column of news genres in the data"]},{"cell_type":"markdown","metadata":{},"source":["Use *dot notation* and a *column name* with `sample()`` to sample values in that column (Series).\n","\n","**Remember**: The dataframe stays the same because we are not altering `df`."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1649709032655,"user":{"displayName":"Chris Lindgren","userId":"04896671906605823933"},"user_tz":240},"id":"pnlQ_4KWlgtF","outputId":"ac8f9318-bd51-4f5e-ed1e-4009c90f6038"},"outputs":[],"source":["df.category.sample(5)"]},{"cell_type":"markdown","metadata":{},"source":["Now, let's read and understand the news genre categories (politics, entertainment, etc.) in the dataset, as well as their distribution."]},{"cell_type":"markdown","metadata":{},"source":["##### 2.2.3.1 Examine the `category` counts and values"]},{"cell_type":"markdown","metadata":{},"source":["We can use the `len()` and `set()` functions in Python to isolate the unique number of values in a column. \n","\n","`len()` by itself would provide a length of *all values combined*, even if the value is repeated. By using `set()` first on the column, we can tell Python to reduce the column to unique set of values. Then, it will count that unique set with `len()`."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1649709033022,"user":{"displayName":"Chris Lindgren","userId":"04896671906605823933"},"user_tz":240},"id":"WmxpY6RzlgtG","outputId":"bc1ff705-4ee4-4b03-a2a9-c35ff4e973ca"},"outputs":[],"source":["len(\n","  set(df['category'].values)\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1649709033023,"user":{"displayName":"Chris Lindgren","userId":"04896671906605823933"},"user_tz":240},"id":"su_oZCw3lgtH","outputId":"9f85a984-6255-4a13-cc3e-a7e0f72c4aa1"},"outputs":[],"source":["set(\n","  df['category'].values\n",")"]},{"cell_type":"markdown","metadata":{"id":"yoLCJSPvlgtH"},"source":["##### 2.2.3.2 `category` by count"]},{"cell_type":"markdown","metadata":{},"source":["Let's review the distribution of categories, since this could impact the model.\n","\n","In the code below, we tell Python to:\n","\n","1. Select the `category` column,\n","2. Count the values of each value in the column with `value_counts()`,\n","3. Plot the results with `plot` and provide the value `bar` for its parameter `kind`"]},{"cell_type":"markdown","metadata":{},"source":["##### 2.2.3.3 *EXERCISE* -- Notes on the distribution of the data based on the `category` column"]},{"cell_type":"markdown","metadata":{},"source":["- ***ENTER YOUR FIRST OBSERVATION HERE***\n","  - Question: How might this impact our model's output?\n","  - ***YOUR RESPONSE HERE***\n","- ***ENTER YOUR SECOND OBSERVATION HERE***\n","  - Question: How might this impact our model's output?\n","  - ***YOUR RESPONSE HERE***\n","- ***ENTER MORE OBSERVATIONS***\n","  - Question: How might this impact our model's output?\n","  - ***YOUR RESPONSE HERE***"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":359},"executionInfo":{"elapsed":430,"status":"ok","timestamp":1649709033450,"user":{"displayName":"Chris Lindgren","userId":"04896671906605823933"},"user_tz":240},"id":"YIC6dO5tlgtH","outputId":"094d2b3e-d436-4025-e984-db260c7e85b1"},"outputs":[],"source":["df['category'].value_counts().plot(kind='bar')"]},{"cell_type":"markdown","metadata":{},"source":["#### 2.2.4 Describe and examine the `short_description` column"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.short_description.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df['sd_length'] = df.short_description.apply(len)\n","df[['short_description','sd_length']].sample(5)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.sd_length.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.sd_length.hist(\n","  figsize=(12,6),\n","  color='#86bf91',\n",")"]},{"cell_type":"markdown","metadata":{},"source":["##### 2.2.4.1 *EXERCISE* -- Notes on the `sd_length` column"]},{"cell_type":"markdown","metadata":{},"source":["- ***ENTER YOUR FIRST OBSERVATION HERE***\n","  - Question: How might this impact our model's output?\n","  - ***YOUR RESPONSE HERE***\n","- ***ENTER YOUR SECOND OBSERVATION HERE***\n","  - Question: How might this impact our model's output?\n","  - ***YOUR RESPONSE HERE***\n","- ***ENTER MORE OBSERVATIONS***\n","  - Question: How might this impact our model's output?\n","  - ***YOUR RESPONSE HERE***"]},{"cell_type":"markdown","metadata":{"id":"fOHLfyNmlgtH"},"source":["## 3. Process the Data for Classification"]},{"cell_type":"markdown","metadata":{},"source":["Some of the fields will be useful to use for the classification task.\n","\n","1. `headline`: Since we're writing a model to classify headlines, this column is integral to our goal.\n","2. `short_description`: This column includes a short descrtiption of the story, which will provide potentially helpful contextual information to make a better model for our genre classifier. We'll see!\n","3. `tokenized_url`: This column includes information too! Different parts of URLs often capture how people have organized and classified information. So, the URL may also provide potentially helpful contextual information to make a better model. Again, we'll see!\n","\n","The code below creates 3 new columns for that task.\n","\n","There's a lot to unpack below, but it basically\n","\n","1. normalizes the URL for each article via the `tokenize_url()` function, which strips the URL down to the portion where the URL conveys article-specific information. Then, \n","2. it creates 3 new columns that are combined with the available data."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1698,"status":"ok","timestamp":1649709035143,"user":{"displayName":"Chris Lindgren","userId":"04896671906605823933"},"user_tz":240},"id":"-ahlrsvSlgtH","outputId":"cd838282-f8ef-491c-8934-485512ea3499"},"outputs":[],"source":["def tokenize_url(url:str):\n","    url=url.replace(\"https://www.huffingtonpost.com/entry/\",\"\")\n","    url=re.sub(\"(\\W|_)+\",\" \",url)\n","    return url\n","\n","df['tokenized_url']=df['link'].apply(lambda x:tokenize_url(x))\n","\n","#just the description\n","df['text_desc'] = df['short_description']\n","\n","#description + headline\n","df['text_desc_headline'] = df['short_description'] + ' '+ df['headline']\n","\n","#description + headline + tokenized url\n","df['text_desc_headline_url'] = df['short_description'] + ' '+ df['headline']+\" \" + df['tokenized_url']\n","\n","df.info()\n"]},{"cell_type":"markdown","metadata":{},"source":["Let's sample the new row with the URL info to see what it includes.\n","\n","##### EXERCISE -- Notes on the on the `text_desc_headline_url` column\n","\n","- ***ENTER YOUR OBSERVATION HERE***\n","  - Question: How might this impact our model's output?\n","  - ***YOUR RESPONSE HERE***\n","- ***ENTER MORE OBSERVATIONS***\n","  - Question: How might this impact our model's output?\n","  - ***YOUR RESPONSE HERE***"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df['text_desc_headline_url'].sample(5).values.tolist()"]},{"cell_type":"markdown","metadata":{"id":"L6kJ6vitlgtI"},"source":["## 4. Train a Logistic Regression Model"]},{"cell_type":"markdown","metadata":{},"source":["### 4.1 What's a Logistic Regression Model?"]},{"cell_type":"markdown","metadata":{},"source":["Logistic regression returns a probability that something may or may not happen or \"be\". We can use this probability as a single value to assess the likelihood of the event/being, such as \"This X headline is of Y news genre\".\n","\n","So, let's say our LR model returns a value of 0.990 for a particular headline's news genre as being \"POLITICS\". This probability score is very likely to accurately predict that this headline is indeed an article about POLITICS. Conversely, another headline with a prediction score of 0.005 on that same logistic regression model is very likely not about POLITICS. Yet, what about a headline with a prediction score of 0.6? \n","\n","In this lesson, our LR model uses those probability estimates as a binary category. To do so, we must decide what's called a \"classification threshold\" or \"decision threshold\". Any value above that threshold indicates a headline is about POLITICS, and any value below the threshold indicates that the headline is not POLITICS, but some other news genre.\n","\n","The default decision threshold in the scikit-learn code library that we will use is 0.5. But, this library also enables us to \"tune\" the LR model based on our problem-dependency / context, as well as take the best/top probability score to predict the news genre of the input headline."]},{"cell_type":"markdown","metadata":{},"source":["### 4.2 How to Train a Drago... Erm ... a LR Model!"]},{"cell_type":"markdown","metadata":{},"source":["#### 4.2.1 *From Words to Numbers!* - Transform a corpus of words into matrix of integers with `CountVectorizer()`"]},{"cell_type":"markdown","metadata":{},"source":["**Before you review the many functions defined in section 4.2 below**, functions that will automate the modeling process, I want you to understand a very important first step: how we get from word label data to numbers for the model. After all, this is \"Writing & Digital Media\" **:-)**\n","\n","So, first things first, we're working with text, so strings and characters. But, a logistic regression model requires numbers. So, we need to extract the features of our corpus and transform them into a data type that can be quantified.\n","\n","`CountVectorizer()` to the rescue!"]},{"cell_type":"markdown","metadata":{},"source":["To better understand how scikit-learn's built-in functions transform the textual data for us, I provide us with a tiny tiny case of a list of sentences, which transforms our textural data into a **sparse matrix** of values that indicate the frequency of times the word (feature/token) is present in a unit of content (document). \n","\n","In this tiny case below, our documents are pretty short strings in a list. Those could be much much bigger, and really it should be for this type of modeling. But, for example purposes, let's see how the matrix logs how many instances a word shows up in each document. \n","\n","Each row is the document, (i.e., sentence, in this case), while each column is the feature (word/token). So the values at the intersection of the row and column in the matrix report the raw frequency count (integer)."]},{"cell_type":"code","execution_count":528,"metadata":{},"outputs":[],"source":["# Tiny weeny bitty small corpus :-)\n","# 3 documents (strings) in a list\n","list_test_text_1 = [\n","  'Hello my name is chris we are here to do python python python python', \n","  'chris this is my python notebook you know python',\n","  'The python notebook is a chore chris no chris really it is a chore chore chore'\n","]\n","\n","# Instantiate an sklearn CountVectorizer (CV) objects\n","test_cv_object_binary = CountVectorizer(binary=True) #binary values\n","test_cv_object_counts = CountVectorizer(binary=False) #regular frequency counts\n","# Instantiate an sklearn TfidfVectorizer objects\n","test_tfidf_vectorizer_normed = TfidfVectorizer() # normed tfidf scores\n","test_tfidf_vectorizer = TfidfVectorizer(norm=None, smooth_idf=False) #not normed tfidf scores\n","\n","# Use the vectorizers' fit_transform() function to conduct the transformation of words into particular quantified values\n","test_count_matrix_binary = test_cv_object_binary.fit_transform(list_test_text_1)\n","test_count_matrix_counts = test_cv_object_counts.fit_transform(list_test_text_1)\n","test_tfidf_matrix_normed = test_tfidf_vectorizer_normed.fit_transform(list_test_text_1)\n","test_tfidf_matrix = test_tfidf_vectorizer.fit_transform(list_test_text_1)"]},{"cell_type":"code","execution_count":529,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Binary Data Type: <class 'scipy.sparse._csr.csr_matrix'> \n","Counts Data Type:: <class 'scipy.sparse._csr.csr_matrix'> \n","TFIDF Data Type: <class 'scipy.sparse._csr.csr_matrix'>\n"]}],"source":["# Note the specialized data type for sklearn\n","print(\n","  'Binary Data Type:',type(test_count_matrix_binary),\n","  '\\nCounts Data Type::',type(test_count_matrix_counts),\n","  '\\nTFIDF Data Type:',type(test_tfidf_matrix),\n",")"]},{"cell_type":"markdown","metadata":{},"source":["If I try to print out the sklearn cv matrix, I'll get some basic data structural info."]},{"cell_type":"code","execution_count":550,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(3, 20)\n"]}],"source":["print(test_count_matrix_binary.get_shape())"]},{"cell_type":"markdown","metadata":{},"source":["The matrix has 3 rows (documents/sentences) and 20 columns (unique words/tokens) with values that are dependent on the type of summary: binary, counts, or tfidf.\n","\n","Let's check out the now transformed/quantified data by converting the returned sparse matrices as a more recognizable data type: a pandas dataframe."]},{"cell_type":"code","execution_count":519,"metadata":{},"outputs":[],"source":["def convert_sparse_matrix_to_df(matrix, vectorized_object):\n","  # Convert the matrix into a more basic Python array\n","  count_array = matrix.toarray()\n","\n","  # Use that array as the data in the DF, then nicely use CV or TFIDF vector object's `get_feature_names_out()` method to provide the column names\n","  sparse_matrix_as_df = pd.DataFrame(\n","    data=count_array,\n","    columns=vectorized_object.get_feature_names_out()\n","  )\n","\n","  return sparse_matrix_as_df"]},{"cell_type":"code","execution_count":533,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>are</th>\n","      <th>chore</th>\n","      <th>chris</th>\n","      <th>do</th>\n","      <th>hello</th>\n","      <th>here</th>\n","      <th>is</th>\n","      <th>it</th>\n","      <th>know</th>\n","      <th>my</th>\n","      <th>name</th>\n","      <th>no</th>\n","      <th>notebook</th>\n","      <th>python</th>\n","      <th>really</th>\n","      <th>the</th>\n","      <th>this</th>\n","      <th>to</th>\n","      <th>we</th>\n","      <th>you</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   are  chore  chris  do  hello  here  is  it  know  my  name  no  notebook  \\\n","0    1      0      1   1      1     1   1   0     0   1     1   0         0   \n","1    0      0      1   0      0     0   1   0     1   1     0   0         1   \n","2    0      1      1   0      0     0   1   1     0   0     0   1         1   \n","\n","   python  really  the  this  to  we  you  \n","0       1       0    0     0   1   1    0  \n","1       1       0    0     1   0   0    1  \n","2       1       1    1     0   0   0    0  "]},"execution_count":533,"metadata":{},"output_type":"execute_result"}],"source":["# Binary\n","convert_sparse_matrix_to_df(test_count_matrix_binary, test_cv_object_binary)"]},{"cell_type":"code","execution_count":551,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>are</th>\n","      <th>chore</th>\n","      <th>chris</th>\n","      <th>do</th>\n","      <th>hello</th>\n","      <th>here</th>\n","      <th>is</th>\n","      <th>it</th>\n","      <th>know</th>\n","      <th>my</th>\n","      <th>name</th>\n","      <th>no</th>\n","      <th>notebook</th>\n","      <th>python</th>\n","      <th>really</th>\n","      <th>the</th>\n","      <th>this</th>\n","      <th>to</th>\n","      <th>we</th>\n","      <th>you</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   are  chore  chris  do  hello  here  is  it  know  my  name  no  notebook  \\\n","0    1      0      1   1      1     1   1   0     0   1     1   0         0   \n","1    0      0      1   0      0     0   1   0     1   1     0   0         1   \n","2    0      4      2   0      0     0   2   1     0   0     0   1         1   \n","\n","   python  really  the  this  to  we  you  \n","0       4       0    0     0   1   1    0  \n","1       2       0    0     1   0   0    1  \n","2       1       1    1     0   0   0    0  "]},"execution_count":551,"metadata":{},"output_type":"execute_result"}],"source":["# Counts\n","convert_sparse_matrix_to_df(test_count_matrix_counts, test_cv_object_counts)"]},{"cell_type":"code","execution_count":535,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>are</th>\n","      <th>chore</th>\n","      <th>chris</th>\n","      <th>do</th>\n","      <th>hello</th>\n","      <th>here</th>\n","      <th>is</th>\n","      <th>it</th>\n","      <th>know</th>\n","      <th>my</th>\n","      <th>name</th>\n","      <th>no</th>\n","      <th>notebook</th>\n","      <th>python</th>\n","      <th>really</th>\n","      <th>the</th>\n","      <th>this</th>\n","      <th>to</th>\n","      <th>we</th>\n","      <th>you</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2.098612</td>\n","      <td>0.000000</td>\n","      <td>1.0</td>\n","      <td>2.098612</td>\n","      <td>2.098612</td>\n","      <td>2.098612</td>\n","      <td>1.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>1.405465</td>\n","      <td>2.098612</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>4.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>2.098612</td>\n","      <td>2.098612</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>1.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>1.0</td>\n","      <td>0.000000</td>\n","      <td>2.098612</td>\n","      <td>1.405465</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>1.405465</td>\n","      <td>2.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>2.098612</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>2.098612</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.000000</td>\n","      <td>8.394449</td>\n","      <td>2.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>2.0</td>\n","      <td>2.098612</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>2.098612</td>\n","      <td>1.405465</td>\n","      <td>1.0</td>\n","      <td>2.098612</td>\n","      <td>2.098612</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        are     chore  chris        do     hello      here   is        it  \\\n","0  2.098612  0.000000    1.0  2.098612  2.098612  2.098612  1.0  0.000000   \n","1  0.000000  0.000000    1.0  0.000000  0.000000  0.000000  1.0  0.000000   \n","2  0.000000  8.394449    2.0  0.000000  0.000000  0.000000  2.0  2.098612   \n","\n","       know        my      name        no  notebook  python    really  \\\n","0  0.000000  1.405465  2.098612  0.000000  0.000000     4.0  0.000000   \n","1  2.098612  1.405465  0.000000  0.000000  1.405465     2.0  0.000000   \n","2  0.000000  0.000000  0.000000  2.098612  1.405465     1.0  2.098612   \n","\n","        the      this        to        we       you  \n","0  0.000000  0.000000  2.098612  2.098612  0.000000  \n","1  0.000000  2.098612  0.000000  0.000000  2.098612  \n","2  2.098612  0.000000  0.000000  0.000000  0.000000  "]},"execution_count":535,"metadata":{},"output_type":"execute_result"}],"source":["# Not Normed TF-IDF\n","convert_sparse_matrix_to_df(test_tfidf_matrix, test_tfidf_vectorizer)"]},{"cell_type":"code","execution_count":536,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>are</th>\n","      <th>chore</th>\n","      <th>chris</th>\n","      <th>do</th>\n","      <th>hello</th>\n","      <th>here</th>\n","      <th>is</th>\n","      <th>it</th>\n","      <th>know</th>\n","      <th>my</th>\n","      <th>name</th>\n","      <th>no</th>\n","      <th>notebook</th>\n","      <th>python</th>\n","      <th>really</th>\n","      <th>the</th>\n","      <th>this</th>\n","      <th>to</th>\n","      <th>we</th>\n","      <th>you</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.268634</td>\n","      <td>0.000000</td>\n","      <td>0.158660</td>\n","      <td>0.268634</td>\n","      <td>0.268634</td>\n","      <td>0.268634</td>\n","      <td>0.158660</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.204303</td>\n","      <td>0.268634</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.634638</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.268634</td>\n","      <td>0.268634</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.236251</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.236251</td>\n","      <td>0.000000</td>\n","      <td>0.400008</td>\n","      <td>0.304216</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.304216</td>\n","      <td>0.472502</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.400008</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.400008</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.000000</td>\n","      <td>0.821339</td>\n","      <td>0.242548</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.242548</td>\n","      <td>0.205335</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.205335</td>\n","      <td>0.156162</td>\n","      <td>0.121274</td>\n","      <td>0.205335</td>\n","      <td>0.205335</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        are     chore     chris        do     hello      here        is  \\\n","0  0.268634  0.000000  0.158660  0.268634  0.268634  0.268634  0.158660   \n","1  0.000000  0.000000  0.236251  0.000000  0.000000  0.000000  0.236251   \n","2  0.000000  0.821339  0.242548  0.000000  0.000000  0.000000  0.242548   \n","\n","         it      know        my      name        no  notebook    python  \\\n","0  0.000000  0.000000  0.204303  0.268634  0.000000  0.000000  0.634638   \n","1  0.000000  0.400008  0.304216  0.000000  0.000000  0.304216  0.472502   \n","2  0.205335  0.000000  0.000000  0.000000  0.205335  0.156162  0.121274   \n","\n","     really       the      this        to        we       you  \n","0  0.000000  0.000000  0.000000  0.268634  0.268634  0.000000  \n","1  0.000000  0.000000  0.400008  0.000000  0.000000  0.400008  \n","2  0.205335  0.205335  0.000000  0.000000  0.000000  0.000000  "]},"execution_count":536,"metadata":{},"output_type":"execute_result"}],"source":["# Normed TF-IDF\n","convert_sparse_matrix_to_df(test_tfidf_matrix_normed, test_tfidf_vectorizer_normed)"]},{"cell_type":"markdown","metadata":{},"source":["**What do we notice?**\n","\n","All dataframe/matrices include:\n","\n","- 3 rows == number of documents\n","- 20 columns == Set number of terms/tokens\n","- **Binary** values are either 0 or 1 \n","- **Counts** values are the *frequency* of times a word shows up in a document\n","- **TFIDF** values are a type of *weighted average* score per word in a document"]},{"cell_type":"markdown","metadata":{},"source":["##### So what is TFIDF, again?"]},{"cell_type":"markdown","metadata":{},"source":["TF-IDF measures how important a *term* is within a *document* relative to a *collection of documents/corpus*. It is comprised of 2 values: _Term Frequency/commonality_ &amp; _Inverse Document Frequency/rarity_. In our test simple case, we have 3 documents (3 lists of strings), wherein the total number of words/tokens across all docs is 36.\n","\n","A term's TF-IDF score is high when it is frequently used in a document **and rarely** in other documents in the collection. Conversely, a term's TF-IDF score is lowered when it is just as frequently a document **and frequently** in other documents in the collection. Consequently, a term's commonality within a document (TF) is balanced by the term's rarity across all documents (IDF), which provides one useful score to measure the importance of a term for a document in the corpus."]},{"cell_type":"markdown","metadata":{},"source":["**Term Frequency**\n","\n","A term's *commonality* -- Number of times the term appears in a document compared to the total number of words in the document. -- TF = Frequency of term in a document / Total number of tokens (39)"]},{"cell_type":"markdown","metadata":{},"source":["<p><mjx-container class=\"MathJax CtxtMenu_Attached_0\" jax=\"CHTML\" display=\"true\" style=\"position: relative;\" tabindex=\"0\" ctxtmenu_counter=\"12\"><mjx-math display=\"true\" style=\"margin-left: 0px; margin-right: 0px;\" class=\"MJX-TEX\" aria-hidden=\"true\"><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D447 TEX-I\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D439 TEX-I\"></mjx-c></mjx-mi><mjx-mo class=\"mjx-n\" space=\"4\"><mjx-c class=\"mjx-c3D\"></mjx-c></mjx-mo><mjx-mfrac space=\"4\"><mjx-frac type=\"d\"><mjx-num><mjx-nstrut type=\"d\"></mjx-nstrut><mjx-mtext class=\"mjx-n\"><mjx-c class=\"mjx-c6E\"></mjx-c><mjx-c class=\"mjx-c75\"></mjx-c><mjx-c class=\"mjx-c6D\"></mjx-c><mjx-c class=\"mjx-c62\"></mjx-c><mjx-c class=\"mjx-c65\"></mjx-c><mjx-c class=\"mjx-c72\"></mjx-c><mjx-c class=\"mjx-c20\"></mjx-c><mjx-c class=\"mjx-c6F\"></mjx-c><mjx-c class=\"mjx-c66\"></mjx-c><mjx-c class=\"mjx-c20\"></mjx-c><mjx-c class=\"mjx-c74\"></mjx-c><mjx-c class=\"mjx-c69\"></mjx-c><mjx-c class=\"mjx-c6D\"></mjx-c><mjx-c class=\"mjx-c65\"></mjx-c><mjx-c class=\"mjx-c73\"></mjx-c><mjx-c class=\"mjx-c20\"></mjx-c><mjx-c class=\"mjx-c74\"></mjx-c><mjx-c class=\"mjx-c68\"></mjx-c><mjx-c class=\"mjx-c65\"></mjx-c><mjx-c class=\"mjx-c20\"></mjx-c><mjx-c class=\"mjx-c74\"></mjx-c><mjx-c class=\"mjx-c65\"></mjx-c><mjx-c class=\"mjx-c72\"></mjx-c><mjx-c class=\"mjx-c6D\"></mjx-c><mjx-c class=\"mjx-c20\"></mjx-c><mjx-c class=\"mjx-c61\"></mjx-c><mjx-c class=\"mjx-c70\"></mjx-c><mjx-c class=\"mjx-c70\"></mjx-c><mjx-c class=\"mjx-c65\"></mjx-c><mjx-c class=\"mjx-c61\"></mjx-c><mjx-c class=\"mjx-c72\"></mjx-c><mjx-c class=\"mjx-c73\"></mjx-c><mjx-c class=\"mjx-c20\"></mjx-c><mjx-c class=\"mjx-c69\"></mjx-c><mjx-c class=\"mjx-c6E\"></mjx-c><mjx-c class=\"mjx-c20\"></mjx-c><mjx-c class=\"mjx-c74\"></mjx-c><mjx-c class=\"mjx-c68\"></mjx-c><mjx-c class=\"mjx-c65\"></mjx-c><mjx-c class=\"mjx-c20\"></mjx-c><mjx-c class=\"mjx-c64\"></mjx-c><mjx-c class=\"mjx-c6F\"></mjx-c><mjx-c class=\"mjx-c63\"></mjx-c><mjx-c class=\"mjx-c75\"></mjx-c><mjx-c class=\"mjx-c6D\"></mjx-c><mjx-c class=\"mjx-c65\"></mjx-c><mjx-c class=\"mjx-c6E\"></mjx-c><mjx-c class=\"mjx-c74\"></mjx-c></mjx-mtext></mjx-num><mjx-dbox><mjx-dtable><mjx-line type=\"d\"></mjx-line><mjx-row><mjx-den><mjx-dstrut type=\"d\"></mjx-dstrut><mjx-mtext class=\"mjx-n\"><mjx-c class=\"mjx-c74\"></mjx-c><mjx-c class=\"mjx-c6F\"></mjx-c><mjx-c class=\"mjx-c74\"></mjx-c><mjx-c class=\"mjx-c61\"></mjx-c><mjx-c class=\"mjx-c6C\"></mjx-c><mjx-c class=\"mjx-c20\"></mjx-c><mjx-c class=\"mjx-c6E\"></mjx-c><mjx-c class=\"mjx-c75\"></mjx-c><mjx-c class=\"mjx-c6D\"></mjx-c><mjx-c class=\"mjx-c62\"></mjx-c><mjx-c class=\"mjx-c65\"></mjx-c><mjx-c class=\"mjx-c72\"></mjx-c><mjx-c class=\"mjx-c20\"></mjx-c><mjx-c class=\"mjx-c6F\"></mjx-c><mjx-c class=\"mjx-c66\"></mjx-c><mjx-c class=\"mjx-c20\"></mjx-c><mjx-c class=\"mjx-c74\"></mjx-c><mjx-c class=\"mjx-c65\"></mjx-c><mjx-c class=\"mjx-c72\"></mjx-c><mjx-c class=\"mjx-c6D\"></mjx-c><mjx-c class=\"mjx-c73\"></mjx-c><mjx-c class=\"mjx-c20\"></mjx-c><mjx-c class=\"mjx-c69\"></mjx-c><mjx-c class=\"mjx-c6E\"></mjx-c><mjx-c class=\"mjx-c20\"></mjx-c><mjx-c class=\"mjx-c74\"></mjx-c><mjx-c class=\"mjx-c68\"></mjx-c><mjx-c class=\"mjx-c65\"></mjx-c><mjx-c class=\"mjx-c20\"></mjx-c><mjx-c class=\"mjx-c64\"></mjx-c><mjx-c class=\"mjx-c6F\"></mjx-c><mjx-c class=\"mjx-c63\"></mjx-c><mjx-c class=\"mjx-c75\"></mjx-c><mjx-c class=\"mjx-c6D\"></mjx-c><mjx-c class=\"mjx-c65\"></mjx-c><mjx-c class=\"mjx-c6E\"></mjx-c><mjx-c class=\"mjx-c74\"></mjx-c></mjx-mtext></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac></mjx-math><mjx-assistive-mml unselectable=\"on\" display=\"block\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>T</mi><mi>F</mi><mo>=</mo><mfrac><mtext>number of times the term appears in the document</mtext><mtext>total number of terms in the document</mtext></mfrac></math></mjx-assistive-mml></mjx-container></p>"]},{"cell_type":"code","execution_count":552,"metadata":{},"outputs":[{"data":{"text/plain":["39"]},"execution_count":552,"metadata":{},"output_type":"execute_result"}],"source":["test_total_terms_in_collection = len( (list_test_text_1[0] +' '+ list_test_text_1[1] +' '+ list_test_text_1[2]).split() )\n","test_total_terms_in_collection"]},{"cell_type":"code","execution_count":553,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["TF 'python': 0.10256410256410256 \n","TF 'chore': 0.10256410256410256\n"]}],"source":["# Term Frequency (TF) for 'python' in second document, i.e., string in the list\n","tf_python_d1 = 4 / test_total_terms_in_collection\n","tf_chore_d3 = 4 / test_total_terms_in_collection\n","print(\n","  'TF \\'python\\':', tf_python_d1,\n","  '\\nTF \\'chore\\':', tf_chore_d3,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["**Inverse Document Frequency**: A term's *rarity* -- IDF of a term reflects the proportion of documents in the corpus that contain the term. Words unique to a small percentage of documents (e.g., technical jargon terms) receive higher importance values than words common across all documents (e.g., a, the, and)."]},{"cell_type":"markdown","metadata":{},"source":["<p><mjx-container class=\"MathJax CtxtMenu_Attached_0\" jax=\"CHTML\" display=\"true\" style=\"position: relative;\" tabindex=\"0\" ctxtmenu_counter=\"13\"><mjx-math display=\"true\" style=\"margin-left: 0px; margin-right: 0px;\" class=\"MJX-TEX\" aria-hidden=\"true\"><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D43C TEX-I\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D437 TEX-I\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D439 TEX-I\"></mjx-c></mjx-mi><mjx-mo class=\"mjx-n\" space=\"4\"><mjx-c class=\"mjx-c3D\"></mjx-c></mjx-mo><mjx-mi class=\"mjx-i\" space=\"4\"><mjx-c class=\"mjx-c1D459 TEX-I\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D45C TEX-I\"></mjx-c></mjx-mi><mjx-mi class=\"mjx-i\"><mjx-c class=\"mjx-c1D454 TEX-I\"></mjx-c></mjx-mi><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c28\"></mjx-c></mjx-mo><mjx-mfrac><mjx-frac type=\"d\"><mjx-num><mjx-nstrut type=\"d\"></mjx-nstrut><mjx-mtext class=\"mjx-n\"><mjx-c class=\"mjx-c6E\"></mjx-c><mjx-c class=\"mjx-c75\"></mjx-c><mjx-c class=\"mjx-c6D\"></mjx-c><mjx-c class=\"mjx-c62\"></mjx-c><mjx-c class=\"mjx-c65\"></mjx-c><mjx-c class=\"mjx-c72\"></mjx-c><mjx-c class=\"mjx-c20\"></mjx-c><mjx-c class=\"mjx-c6F\"></mjx-c><mjx-c class=\"mjx-c66\"></mjx-c><mjx-c class=\"mjx-c20\"></mjx-c><mjx-c class=\"mjx-c74\"></mjx-c><mjx-c class=\"mjx-c68\"></mjx-c><mjx-c class=\"mjx-c65\"></mjx-c><mjx-c class=\"mjx-c20\"></mjx-c><mjx-c class=\"mjx-c64\"></mjx-c><mjx-c class=\"mjx-c6F\"></mjx-c><mjx-c class=\"mjx-c63\"></mjx-c><mjx-c class=\"mjx-c75\"></mjx-c><mjx-c class=\"mjx-c6D\"></mjx-c><mjx-c class=\"mjx-c65\"></mjx-c><mjx-c class=\"mjx-c6E\"></mjx-c><mjx-c class=\"mjx-c74\"></mjx-c><mjx-c class=\"mjx-c73\"></mjx-c><mjx-c class=\"mjx-c20\"></mjx-c><mjx-c class=\"mjx-c69\"></mjx-c><mjx-c class=\"mjx-c6E\"></mjx-c><mjx-c class=\"mjx-c20\"></mjx-c><mjx-c class=\"mjx-c74\"></mjx-c><mjx-c class=\"mjx-c68\"></mjx-c><mjx-c class=\"mjx-c65\"></mjx-c><mjx-c class=\"mjx-c20\"></mjx-c><mjx-c class=\"mjx-c63\"></mjx-c><mjx-c class=\"mjx-c6F\"></mjx-c><mjx-c class=\"mjx-c72\"></mjx-c><mjx-c class=\"mjx-c70\"></mjx-c><mjx-c class=\"mjx-c75\"></mjx-c><mjx-c class=\"mjx-c73\"></mjx-c></mjx-mtext></mjx-num><mjx-dbox><mjx-dtable><mjx-line type=\"d\"></mjx-line><mjx-row><mjx-den><mjx-dstrut type=\"d\"></mjx-dstrut><mjx-mtext class=\"mjx-n\"><mjx-c class=\"mjx-c6E\"></mjx-c><mjx-c class=\"mjx-c75\"></mjx-c><mjx-c class=\"mjx-c6D\"></mjx-c><mjx-c class=\"mjx-c62\"></mjx-c><mjx-c class=\"mjx-c65\"></mjx-c><mjx-c class=\"mjx-c72\"></mjx-c><mjx-c class=\"mjx-c20\"></mjx-c><mjx-c class=\"mjx-c6F\"></mjx-c><mjx-c class=\"mjx-c66\"></mjx-c><mjx-c class=\"mjx-c20\"></mjx-c><mjx-c class=\"mjx-c64\"></mjx-c><mjx-c class=\"mjx-c6F\"></mjx-c><mjx-c class=\"mjx-c63\"></mjx-c><mjx-c class=\"mjx-c75\"></mjx-c><mjx-c class=\"mjx-c6D\"></mjx-c><mjx-c class=\"mjx-c65\"></mjx-c><mjx-c class=\"mjx-c6E\"></mjx-c><mjx-c class=\"mjx-c74\"></mjx-c><mjx-c class=\"mjx-c73\"></mjx-c><mjx-c class=\"mjx-c20\"></mjx-c><mjx-c class=\"mjx-c69\"></mjx-c><mjx-c class=\"mjx-c6E\"></mjx-c><mjx-c class=\"mjx-c20\"></mjx-c><mjx-c class=\"mjx-c74\"></mjx-c><mjx-c class=\"mjx-c68\"></mjx-c><mjx-c class=\"mjx-c65\"></mjx-c><mjx-c class=\"mjx-c20\"></mjx-c><mjx-c class=\"mjx-c63\"></mjx-c><mjx-c class=\"mjx-c6F\"></mjx-c><mjx-c class=\"mjx-c72\"></mjx-c><mjx-c class=\"mjx-c70\"></mjx-c><mjx-c class=\"mjx-c75\"></mjx-c><mjx-c class=\"mjx-c73\"></mjx-c><mjx-c class=\"mjx-c20\"></mjx-c><mjx-c class=\"mjx-c63\"></mjx-c><mjx-c class=\"mjx-c6F\"></mjx-c><mjx-c class=\"mjx-c6E\"></mjx-c><mjx-c class=\"mjx-c74\"></mjx-c><mjx-c class=\"mjx-c61\"></mjx-c><mjx-c class=\"mjx-c69\"></mjx-c><mjx-c class=\"mjx-c6E\"></mjx-c><mjx-c class=\"mjx-c20\"></mjx-c><mjx-c class=\"mjx-c74\"></mjx-c><mjx-c class=\"mjx-c68\"></mjx-c><mjx-c class=\"mjx-c65\"></mjx-c><mjx-c class=\"mjx-c20\"></mjx-c><mjx-c class=\"mjx-c74\"></mjx-c><mjx-c class=\"mjx-c65\"></mjx-c><mjx-c class=\"mjx-c72\"></mjx-c><mjx-c class=\"mjx-c6D\"></mjx-c></mjx-mtext></mjx-den></mjx-row></mjx-dtable></mjx-dbox></mjx-frac></mjx-mfrac><mjx-mo class=\"mjx-n\"><mjx-c class=\"mjx-c29\"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable=\"on\" display=\"block\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><mi>I</mi><mi>D</mi><mi>F</mi><mo>=</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy=\"false\">(</mo><mfrac><mtext>number of the documents in the corpus</mtext><mtext>number of documents in the corpus contain the term</mtext></mfrac><mo stretchy=\"false\">)</mo></math></mjx-assistive-mml></mjx-container></p>"]},{"cell_type":"code","execution_count":554,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["IDF 'python': 0.0 \n","IDF 'chore': 0.47712125471966244\n"]}],"source":["idf_d1_python = np.log10(3 / 3)\n","idf_d3_chore = np.log10(3 / 1)\n","print(\n","  'IDF \\'python\\':', idf_d1_python,\n","  '\\nIDF \\'chore\\':', idf_d3_chore,\n",")"]},{"cell_type":"code","execution_count":555,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["TF-IDF of 'python' in doc 1 of the collection/corpus: 0.0 \n","TF-IDF of 'chore' in doc 3 of the collection/corpus: 0.04893551330458076\n"]}],"source":["\n","print(\n","  'TF-IDF of \\'python\\' in doc 1 of the collection/corpus:', (tf_python_d1 * idf_d1_python),\n","  '\\nTF-IDF of \\'chore\\' in doc 3 of the collection/corpus:', (tf_chore_d3 * idf_d3_chore),\n",")"]},{"cell_type":"markdown","metadata":{},"source":["##### Summarize `CountVectorizer()` &amp; `TfidfVectorizer()`"]},{"cell_type":"markdown","metadata":{},"source":["In the modeling functions below, the `extract_features()` function uses `CountVectorizer()` and `TfidfVectorizer()` to, well, extract these features from the much larger corpus. \n","\n","They both nicely automate the process for us: writing and calculating at amazing speed and scale. \n","\n","**Can you imagine transforming the data manually?!** I don't think we want to subject ourselves to even such a thought."]},{"cell_type":"markdown","metadata":{},"source":["### 4.3 Modeling functions\n","\n","The functions below help us create a systematic and reproducable workflow to train the data.\n","\n","Be sure to check out my videos that walk through an overview of what they do."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1649709035144,"user":{"displayName":"Chris Lindgren","userId":"04896671906605823933"},"user_tz":240},"id":"TkesOTjPlgtI"},"outputs":[],"source":["def _reciprocal_rank(true_genre_labels: list, machine_predicted_genre_labels: list):\n","    '''\n","    ## Purpose\n","    Compute the reciprocal rank at cutoff k\n","\n","    ## Parameters\n","        - `true_genre_labels` (List): List of actual news genre labels\n","        - `machine_predicted_genre_labels` (List): List of news genre labels predicted by the LR algorithm\n","    \n","    ## Return Values\n","        - `recip_rank` (Float): Reciprocal rank\n","    '''\n","    \n","    # add index to list only if machine predicted label exists in true labels\n","    tp_pos_list = [(idx + 1) for idx, r in enumerate(machine_predicted_genre_labels) if r in true_genre_labels]\n","\n","    recip_rank = 0\n","    if len(tp_pos_list) > 0:\n","        # for reciprocal rank we must find the position of the first **correctly labeled** item\n","        first_pos_list = tp_pos_list[0]\n","        \n","        # recip_rank = 1/rank\n","        recip_rank = 1 / float(first_pos_list)\n","\n","    return recip_rank\n","\n","def compute_mrr_at_k(eval_news_category_items:list):\n","    '''\n","    ## Purpose\n","    `compute_mrr_at_k()`: Computes the MRR (average RR) at cutoff k. In sum, it takes the mean average of all of the reciprocal rank scores among the actual vs. predicted labels. Review this [\"Mean reciprocal rank\" wikipedia article](https://en.wikipedia.org/wiki/Mean_reciprocal_rank) for a simple explainer.\n","    ## Parameters\n","    - `eval_news_category_items` (List): List that contains 2 values\n","        1. String - Actual news genre category\n","        2. List of strings - Predicted news genre category in order by estimated probability to be returned by the model.\n","            - The example below shows how \n","                - `'HEALTHY LIVING'` was the actual label, but it was third in 'reciprocal rank' with a value of 1/3\n","                - `'WORLDPOST'` was the actual label, and it was first in 'reciprocal rank' with a value of 1\n","                \n","                [\n","                    [\n","                        ['HEALTHY LIVING'], ['POLITICS', 'ENTERTAINMENT', 'HEALTHY LIVING']\n","                    ], \n","                    [\n","                        ['WORLDPOST'], ['WORLDPOST', 'MEDIA', 'POLITICS']\n","                    ], \n","                    ...\n","                ]\n","\n","    ## Return Values\n","        - `mean_reciprocal_rank_score` (Float): Mean average reciprocal rank score among the predicted news category in the model\n","    '''\n","    rr_total = 0\n","    \n","    for item in eval_news_category_items:\n","        actual_label = item[0]\n","        pred_label_list = item[1]\n","\n","        # Find the reciprocal rank (RR) for this row\n","        rr_at_k = _reciprocal_rank(actual_label, pred_label_list)\n","\n","        # Add the row's RR to the accruing scores for the entire corpus\n","        rr_total = rr_total + rr_at_k\n","\n","        # Update the Mean Reciprocal Rank (MRR) score with new row value\n","        mean_reciprocal_rank_score = rr_total / 1/float(len(eval_news_category_items))\n","\n","    return mean_reciprocal_rank_score\n","\n","def collect_preds(Y_test, Y_preds):\n","    '''\n","    ## Purpose\n","    Collect all predictions (predicted news genre labels) and ground truth (i.e., actual news genre label)\n","    '''\n","    pred_gold_list = [ [ [Y_test[index]], pred ] for index, pred in enumerate(Y_preds) ]\n","    return pred_gold_list\n","             \n","def compute_accuracy(eval_news_category_items:list):\n","    '''\n","    ## Purpose\n","    `compute_accuracy()`: Compute the overall accuracy score of the model across the training corpus\n","\n","    ## Parameters\n","        - `eval_news_category_items` (List): List that contains 2 values\n","            1. String - Actual news genre category\n","            2. List of strings - Predicted news genre category\n","\n","            Example: [\n","                [\n","                    ['HEALTHY LIVING'], ['POLITICS', 'ENTERTAINMENT', 'HEALTHY LIVING']\n","                ], \n","                [\n","                    ['WORLDPOST'], ['WORLDPOST', 'MEDIA', 'POLITICS']\n","                ], \n","                ...\n","            ]\n","    ## Return Values\n","        - `news_cat_prediction_accuracy` (Float): Percentage of accurately predicted news category in the model\n","    '''\n","    correct_news_cat = 0\n","    \n","    for news_genre_cat in eval_news_category_items:\n","        true_pred = news_genre_cat[0]\n","        machine_pred = set(news_genre_cat[1])\n","        \n","        for news_cat in true_pred:\n","            if news_cat in machine_pred:\n","                correct_news_cat += 1\n","                break\n","    \n","    news_cat_prediction_accuracy = correct_news_cat / float(len(eval_news_category_items))\n","    return news_cat_prediction_accuracy\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":183,"status":"ok","timestamp":1649709035322,"user":{"displayName":"Chris Lindgren","userId":"04896671906605823933"},"user_tz":240},"id":"My5nLBMXlgtI"},"outputs":[],"source":["logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n","\n","def extract_features(df, field, training_data, testing_data, type='binary'):\n","    '''\n","    ## Purpose\n","    `extract_features()`: Extract features using different method types: binary, counts, or TF-IDF\n","\n","    ### If BINARY Features\n","    Creates a new `CountVectorizer()` method object, which converts a collection of text documents to a matrix of binary token counts per document. In other words, \n","    - `1` == the feature is represented in the document\n","    - `0` == the feature is not represented in the doc\n","    \n","    Logistic regression involves vectorizing weighted averages of these tokens.\n","\n","    ### If COUNT Features\n","    Creates a new `CountVectorizer()` method object, which converts a collection of text documents to a matrix of `n` token counts per document.  In other words, \n","    - `5` == the feature is represented 5 times in the document\n","    - `25` == the feature is represented 25 times in the document\n","    - `0` == the feature is not represented in the doc\n","    \n","    Logistic regression involves vectorizing weighted averages of these tokens.\n","\n","    ### If TF-IDF Features\n","    Creates a new `CountVectorizer()` method object, which converts a collection of text documents to a matrix of `n` token counts per document.  In other words, \n","    - `5` == the feature is represented 5 times in the document\n","    - `25` == the feature is represented 25 times in the document\n","    - `0` == the feature is not represented in the doc\n","    \n","    Logistic regression involves vectorizing weighted averages of these tokens.\n","    '''\n","    \n","    logging.info(\"Extracting features and creating vocabulary...\")\n","\n","    '''\n","        BINARY and COUNTS PROCESSES WILL DO THE FOLLOWING:\n","\n","        sklearn's CountVectorizer() will convert text to numerical data.\n","    '''\n","    \n","    if 'binary' in type:\n","        \n","        # BINARY FEATURE REPRESENTATION\n","        # Creates a new CountVectorizer() method object, which can help us use built-in functions that convert a collection of text documents to a matrix of token counts. **REMEMBER** that logistic regression involves vectorizing weighted averages of these tokens.\n","        # NOTE: `max_df` == \"Maximum Document Frequency. It enables us to programmatically ignore frequently occuring words, e.g., articles like 'a' or 'the'. `max_df` reviews how many documents contain the word, and if it exceeds the max_df threshold then it is eliminated from the sparse matrix. Below we set the threshold to 95%.\n","        cv = CountVectorizer(binary=True, max_df=0.95)\n","        # CountVectorizer()'s fit_transform() uses the training_data to learn the vocabulary dictionary and return document-term matrix.\n","        cv.fit_transform(training_data[field].values)\n","        # CountVectorizer()'s transform() \n","        train_feature_set = cv.transform(training_data[field].values)\n","        test_feature_set = cv.transform(testing_data[field].values)\n","        \n","        return train_feature_set,test_feature_set,cv\n","  \n","    elif 'counts' in type:\n","        \n","        # COUNT BASED FEATURE REPRESENTATION\n","        cv = CountVectorizer(binary=False, max_df=0.95)\n","        cv.fit_transform(training_data[field].values)\n","        \n","        train_feature_set = cv.transform(training_data[field].values)\n","        test_feature_set = cv.transform(testing_data[field].values)\n","        \n","        return train_feature_set,test_feature_set,cv\n","    \n","    elif 'tfidf':    \n","        \n","        # TF-IDF BASED FEATURE REPRESENTATION\n","        tfidf_vectorizer=TfidfVectorizer(use_idf=True, max_df=0.95)\n","        tfidf_vectorizer.fit_transform(training_data[field].values)\n","        \n","        train_feature_set=tfidf_vectorizer.transform(training_data[field].values)\n","        test_feature_set=tfidf_vectorizer.transform(testing_data[field].values)\n","        \n","        return train_feature_set,test_feature_set,tfidf_vectorizer\n","\n","def get_top_k_predictions(model, X_test, k):\n","    '''\n","    ## Purpose\n","    `get_top_k_predictions()`: Uses the input trained LogisticRegression model to return the news genre class/category with the top estimated probability score.\n","    ## Parameters\n","    - `model` (LogisticRegression()): Trained model scikit-learn object\n","    - `X_test` (pandas DataFrame): Sampled test data set returned by `training_test_split()` in the `training_model()` function\n","    - `k` (Integer): Number of top categories (news genres) to return based on the estimated probability to predict the news genre\n","    ## Return Value(s)\n","    - `preds` (List of list): A list within a list of the top k retruned news categories. For example:\n","        - `preds` is `[['SCIENCE', 'HEALTHY LIVING', 'GREEN']]` for an article with the headline of `\"Exercise in space keeps astronauts from fainting when they return to Earth, study says\"` and `k=3`\n","    '''\n","    \n","    # get probabilities instead of predicted labels, since we want to collect top 3\n","    probs = model.predict_proba(X_test)\n","\n","    # GET TOP K PREDICTIONS BY PROB - note these are just index\n","    best_n = np.argsort(probs, axis=1)[:,-k:]\n","    \n","    # GET CATEGORY OF PREDICTIONS\n","    preds = [[model.classes_[predicted_cat] for predicted_cat in prediction] for prediction in best_n]\n","    \n","    preds = [ item[::-1] for item in preds]\n","    \n","    return preds\n","   \n","def train_model(df, field=\"text_desc\", feature_rep=\"binary\", top_k=3):\n","    '''\n","    ## Purpose\n","    train_model() is the main controller function that conducts the following modeling procedure: \n","        \n","    1. Create X data (List) by splitting the data to create two sampled sets: 1) for training, and 2) for testing.\n","    2. Create Y data (List) by assigning the actual (ground truth) labels\n","    3. Extract the features for the model to use, based on the chosen feature representation: binary vs. TF-IDF\n","    4. Fit, i.e., train, the logistic regression classifier model with scikit-learn's `LogisticRegression()` object\n","    5. Retrieve the evaluation items, e.g., the actual labels (ground truths) and predicted labels (list of top `k` number of estimated probable predicted categories)\n","    6. Use the evaluation iitems to compute the overall accuracy score and mean reciprocal rank score of the model\n","\n","    ## Parameters\n","    - `df` (pandas DataFrame): the complete data set / corpus\n","    - `field` (String): the column name of the feature used to train the model\n","    - `feature_rep` (String): Type of LR analysis set as either \"binary\" or \"count\" or \"tfidf\"\n","    '''\n","    \n","    logging.info(\"Starting model training...\")\n","    \n","    # 1. GET A TRAIN TEST SPLIT (set seed for consistent results)\n","    # train_test_split() from sklearn \"splits arrays or matrices into random train and test subsets.\"\n","    # returns 2 new dataframes: one for training, another for testing the trained model\n","    y = df['category']\n","    x_training_data,x_testing_data = train_test_split(\n","        df,\n","        random_state=2000 #Controls the shuffling applied to the data before applying the split\n","    )\n","\n","    # 2. GET LABELS FROM SPLIT DATA\n","    # Get the category values from each split data returned by #1\n","    Y_train = x_training_data['category'].values\n","    Y_test = x_testing_data['category'].values\n","     \n","    # 3. GET FEATURES\n","    X_train,X_test,feature_transformer = extract_features(\n","        df,\n","        field,\n","        x_training_data,\n","        x_testing_data,\n","        type=feature_rep\n","    )\n","\n","    # INITIALIZE THE LOGISTIC REGRESSION CLASSIFIER OBJECT\n","    logging.info(\"Training a Logistic Regression Model. This may take a few minutes. ...\")\n","    scikit_log_reg = LogisticRegression(\n","        verbose=0, #if you want the LR method to print out all the details, change this 0 to 1\n","        solver='liblinear',\n","        random_state=0,\n","        C=5,\n","        penalty='l2',\n","        max_iter=1000\n","    )\n","    # Create the model by providing the LR object the \n","    model = scikit_log_reg.fit(X_train, Y_train)\n","\n","    # GET TOP K PREDICTIONS\n","    preds = get_top_k_predictions(model, X_test, top_k)\n","    \n","    # GET PREDICTED VALUES AND GROUND TRUTH INTO A LIST OF LISTS - for ease of evaluation\n","    eval_items = collect_preds(Y_test, preds)\n","    \n","    # GET EVALUATION NUMBERS ON TEST SET -- HOW DID WE DO?\n","    logging.info(\"Starting evaluation...\")\n","    simple_mean_avg_correct_prediction_accuracy = compute_accuracy(eval_items)\n","    mean_recip_rank_at_k = compute_mrr_at_k(eval_items)\n","    \n","    logging.info(\"Done training and evaluation.\")\n","\n","    # Return the herein computed model and other values for potential use and exploration\n","    return model,feature_transformer,simple_mean_avg_correct_prediction_accuracy,mean_recip_rank_at_k,X_train,X_test,Y_test,Y_train,preds,eval_items\n"]},{"cell_type":"markdown","metadata":{"id":"9WPZ0JvLlgtI"},"source":["### 4.4 LR Model 1 - Binary features with `text_desc` only"]},{"cell_type":"markdown","metadata":{},"source":["Train the model based on the short description data.\n","\n","**FYI**. Your functions can return multiple variables, if you'd like. \n","\n","If you review the `train_model()` function, it returns multiple variables in a specific order. Those mirror the variables and the comma-separated variables and their desired types below."]},{"cell_type":"markdown","metadata":{},"source":["#### 4.4.1 Enact the Training"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":320275,"status":"ok","timestamp":1649709355588,"user":{"displayName":"Chris Lindgren","userId":"04896671906605823933"},"user_tz":240},"id":"_wWtByIplgtI","outputId":"a2f40cf2-8bca-4187-a9f4-84302d1631d1"},"outputs":[],"source":["# Parameters to configure for our `train_model()` function\n","training_field = 'text_desc' #use the short description only to train a model\n","feature_rep = 'count' # Specify if this model should use a binary approach to the features (0 or 1) or the actual counts created by CountVectorizer()\n","top_k = 3 #tell the model function to return the top 3 'best fits' among the distributed probabilities\n","\n","# Train that supervised ML logistic regression model!\n","model_td_only,transformer_td_only,accuracy_td_only,mrr_at_k_td_only,X_train,X_test,Y_test,Y_train,preds,eval_items = train_model(\n","  df, # full corpus\n","  field=training_field,\n","  feature_rep=feature_rep,\n","  top_k=top_k\n",")"]},{"cell_type":"markdown","metadata":{},"source":["#### Model Assessment Roadmap\n","\n","In the following subsections, you will examine the LR model that you just trained by:\n","\n","1. Print out the overall accuracy scores for the model\n","2. Compare the predictive power of the model across the news genre categories, i.e., *classes*\n","3. Visualize the predictive power per class/category with a \"confusion matrix\" heatmap\n","4. Dig deeper into the categories by comparing some categories' performance against the EDA work on the data set used to train the model"]},{"cell_type":"markdown","metadata":{},"source":["#### 4.4.2 Test the accuracy/performance of the model"]},{"cell_type":"markdown","metadata":{},"source":["##### 4.4.2.1 See the accuracy and Mean Reciprocal Rank Scores"]},{"cell_type":"markdown","metadata":{},"source":["We already computed the overall accuracy and MRR scores, when we trained the model, so let's output them.\n","\n","If you need to refresh yourself on these scores, check out the functions' documentation in the 4.3 section."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(f\"Simple Mean Average Model Accuracy = {accuracy_td_only}\\nMean Reciprocal Rank = {mrr_at_k_td_only}\")"]},{"cell_type":"markdown","metadata":{},"source":["##### 4.4.2.2 Compare Actual Labels (ground truths) with Predicted Labels (with the k threshold)"]},{"cell_type":"markdown","metadata":{},"source":["Before we test our model with headline inputs, we can test its performance with scikit-learn's `predict_proba()` associated function with the output model.\n","\n","This function returns the estmated probability of each categorical label (news genre) in the test sample. With this returned data set, we can visualize the results to see what categories might be mislabeled more often than others and, of course, which categories are performing well.\n","\n","Here is a description of parameter and return data:\n","- `X_test`: sample of the original data to test the trained model\n","- `Y_probability_td_only`: List of the predicted news genre category outputs (String)\n","- `Y_probability_a_td_only`: List of lists of the predicted news genre category based on the estimated probability score\n","  - First list == 1 row from X_test data set\n","    - Lists within that list == Each list contains estimated probability scores (Float) per (31) news genre categories"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Predict the classes on the test data\n","Y_predictions_td_only = model_td_only.predict(X_test)\n","# Predict the classes on the test data, and return the probabilities for each class\n","Y_probability_a_td_only = model_td_only.predict_proba(X_test)"]},{"cell_type":"markdown","metadata":{},"source":["#### 4.4.3 Visualize Actual vs. Prediction with a Confusion Matrix"]},{"cell_type":"markdown","metadata":{},"source":["A confusion matrix helps you organize a direct comparison across all of your possible categories. Essentially, it asks: \n","\n","1. How many times did X (actual) equal the predicted (Y)?\n","2. How many times did X (actual) not equal the predicted (Y)?\n","\n","Another way to phrase the questions is to think of it in action. So, with this data set, there are 31 possible categories (news genres), which means we have 31 categories to test against each other—**that's 961 categories** (31 * 31 = 961)!"]},{"cell_type":"markdown","metadata":{},"source":["A confusion matrix maps what are called the True Positive, False Positive, False Negative, and True Negative prediction outcomes. In the table below, I provide a high-level description of what each of these values represent in the scheme of our LR model on predicting news genres."]},{"cell_type":"markdown","metadata":{},"source":["<div style=\"max-width:700px;\">\n","  <table border=\"1\">\n","  <tbody><tr>\n","    <td style=\"background:lightgreen\" width=\"50%\">\n","      <b>True Positive (TP):</b>\n","      <ul>\n","        <li>Reality: Input SCIENCE headline</li>\n","        <li>LR predicted: \"SCIENCE\"</li>\n","        <li>Outcome: LR's prediction is correct/accurate</li>\n","      </ul>\n","    </td>\n","    <td style=\"background:pink\">\n","      <b>False Positive (FP):</b>\n","      <ul>\n","        <li>Reality: Input is <strong>NOT</strong> a SCIENCE headline</li>\n","        <li>LR predicted: \"SCIENCE\"</li>\n","        <li>Outcome: LR's prediction is incorrect/inaccurate</li>\n","    </ul></td>\n","  </tr>\n","  <tr>\n","    <td style=\"background:pink\">\n","      <b>False Negative (FN):</b>\n","      <ul>\n","        <li>Reality: Input SCIENCE headline</li>\n","        <li>LR predicted: <strong>NOT</strong> \"SCIENCE\"</li>\n","        <li>Outcome: LR's prediction is incorrect/inaccurate</li>\n","      </ul>\n","    </td>\n","    <td style=\"background:lightgreen\">\n","      <b>True Negative (TN):</b>\n","      <ul>\n","        <li>Reality: Input is <strong>NOT</strong> a SCIENCE headline</li>\n","        <li>LR predicted: <strong>NOT</strong> \"SCIENCE\"</li>\n","        <li>Outcome: LR's prediction is correct/accurate</li>\n","      </ul>\n","    </td>\n","  </tr>\n","</tbody></table></div>\n","</div>"]},{"cell_type":"markdown","metadata":{},"source":["Ok, so let's visualize the TPs, FPs, FNs, and TNs as a heatmap with annotated values for quick reference."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["cm_td_only = confusion_matrix(\n","  Y_test, #Sorted List of ground truth (correct/actual) target values\n","  Y_predictions_td_only #Sorted List of estimated targets as returned by a classifier\n",")"]},{"cell_type":"markdown","metadata":{},"source":["Let's be sure to **normalize the confusion matrix values**.\n","\n","The current values in the confusion matrix are simply counts (Integers). But, if we want to visualize the prediction efficacy as a heatmap, we first need to \"normalize\" the values by converting them to a score relative to all of the other comparisons in the same column. For example, `ARTS` has results across 31 categories"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Normalize matrix by columns\n","cm_td_only_normed_by_column = normalize(cm_td_only, axis=0, norm='l1')\n","# Compare the original to the normalized\n","print(\n","  'Simple Counts of the ARTS (first) column:\\n', cm_td_only[0],'\\n\\n',\n","  'Normalized Counts of the ARTS (first) column:\\n',cm_td_only_normed_by_column[0],\n",")"]},{"cell_type":"markdown","metadata":{},"source":["##### 4.4.3.2 Heatmap of the `text_desc` only normalized confusion matrix"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.figure(figsize=(31,14))\n","\n","sns.heatmap(\n","  cm_td_only_normed_by_column, #normalized confusion matrix\n","  annot=True, # add normalized counts of co-occurrences between actual vs. predicted\n","  fmt=\".3f\", # round to thousandths decimal place\n","  linewidths=1, # style choice for row/column lines\n","  square=True, # make the \n","  cmap='Purples',\n","  xticklabels=model_td_only.classes_,\n","  yticklabels=model_td_only.classes_,\n",")\n","\n","# Label the X and Y axes\n","plt.ylabel('Actual Label of News Genre')\n","plt.xlabel('Predicted Label of News Genre')\n","\n","# Let's plug in our overall accuracy measures into the the title\n","all_sample_title = f\"Short Description Count Model\\nOverall Accuracy Score: {accuracy_td_only}\\nMean Reciprocal Rank: {mrr_at_k_td_only}\"\n","plt.title(\n","  all_sample_title,\n","  size=15\n",")\n","\n","# Ok, output time!\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["#### 4.4.4 Explore and Assess Categories/Classes with True Positive Rates, False Positive Rates, etc."]},{"cell_type":"markdown","metadata":{},"source":["Based on the above overall accuracy score and heatmap results per category, you can begin to identify news genre categories (or classes) that may be performing well versus categories that are not performing well. You can use the following code below to explore those performance biases per news genre in the corpus/model.\n","\n","As you work through the code below, take notes as you compare the overall accuracy score of the model with specific optimal threshold scores, and be sure to compare those results against the EDA work that you conducted earlier in the notebook. "]},{"cell_type":"markdown","metadata":{},"source":["Our LR model has 31 classes (news genres), so it's a \"multi-class\" LR model. To help us explore the predictive accuracy across each class, we must first use scikit-learn's `LabelBinarizer()` to enable us to compute the True Positive Rates (TPR) and False Positive Rates (FPR) for each class/category.\n","\n","The 3 variables assigned below will help us process the training and testing data to do so."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["label_binarizer = LabelBinarizer().fit(Y_train)\n","y_onehot_test = label_binarizer.transform(Y_test)\n","y_onehot_test.shape  # (n_samples, n_classes)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["y_onehot_test"]},{"cell_type":"markdown","metadata":{},"source":["##### ROC Curve & AUC"]},{"cell_type":"markdown","metadata":{},"source":["An *ROC curve* (Receiver Operating Characteristic curve) is a graph showing the performance of a classification model at all classification thresholds. This curve plots the False Positive (x) and True Positive (y) rates at their different classification thresholds to help us visualize the performance of the model at different potential thresholds. In effect, this visual can help you isolate the performance of some classes over others in relationship with the original EDA work that you have conducted.\n","\n","You can also use ROC curve plots to see how when you lower the classification threshold, the model will classify more items as positive and increase both False Positives and True Positives.\n","\n","Also, AUC stands for Area Under the Curve. It aggregates all of the probable classification thresholds as a value between 0 and 1. \n","- 0 == The model is wrong 100% of the time\n","- 1 == The model is correct 100% of the time\n","\n","Think of it like the spectrum between 0 and 1 below, where the random positive predictions are positioned to the right of the random negative predictions. So, if the AUC score is `0.714`, then the probability that `p` is positioned to the right of `n` is `71.4%`.\n","\n","```\n","Ouput of the LR Model\n","n = Actual Negative Prediction\n","p = Actual Positive Prediction\n","\n"," nnnnnnnnnnnnnnnnnnnnnpnnnnpnnpnppnnpnppppppppppppp\n","|--------------------------------------------------|\n","0                                                  1\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def roc_curve_per_category(list_classes):\n","  '''\n","  ## Purpose\n","  `roc_curve_per_category`: Combine all of the ROC () values for each category in one DataFrame\n","\n","  ## Parameters\n","  - `list_classes` (List): A list of the classes in the trained model\n","\n","  ## Return Value(s)\n","  - `df_fpr_tpr` (pandas DataFrame): Dataframe with the following values per row:\n","    - `'Class'`: Class from the model for instance\n","    - `'FPR'`: False Positive Rate for instance\n","    - `'TPR'`: True Positive Rate for instance\n","    - `'Threshold_Value'`: Threshold Value for instance\n","    - `'Threshold_Optimal'`: Optimal threshold value for the class as a whole\n","    - `'GMean_Optimal'`: Optimal GMean value for the class as a whole\n","    - `'FPR_Optimal'`: Optimal FPR value for the class as a whole\n","    - `'TPR_Optimal'`: Optimal TPR value for the class as a whole\n","  '''\n","  list_dicts_classes_roc = []\n","\n","  for class_cat in list_classes:\n","    # get class category (news genre)\n","    class_id = np.flatnonzero(label_binarizer.classes_ == class_cat)[0]\n","    Y_probability_a_td_only[:, class_id]\n","\n","    fpr, tpr, thresholds = roc_curve(y_onehot_test[:, class_id], Y_probability_a_td_only[:, class_id])\n","    \n","    # Calculate the Geometric-Mean\n","    geometric_mean = np.sqrt(tpr * (1 - fpr))\n","    \n","    # Find the optimal threshold\n","    index = np.argmax(geometric_mean)\n","    threshold_optimal = round(thresholds[index], ndigits=4)\n","    gmean_optimal = round(geometric_mean[index], ndigits=4)\n","    fpr_optimal = round(fpr[index], ndigits=4)\n","    tpr_optimal = round(tpr[index], ndigits=4)\n","\n","    for i in range(0, len(fpr)):\n","      list_dicts_classes_roc.append({\n","        'Class': class_cat,\n","        'FPR': fpr[i],\n","        'TPR': tpr[i],\n","        'Threshold_Value': thresholds[i],\n","        'Threshold_Optimal': threshold_optimal,\n","        'GMean_Optimal': gmean_optimal,\n","        'FPR_Optimal': fpr_optimal,\n","        'TPR_Optimal': tpr_optimal,\n","      })\n","\n","  df_fpr_tpr = pd.DataFrame(list_dicts_classes_roc)\n","\n","  return df_fpr_tpr"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_all_classes_roc_values = roc_curve_per_category(model_td_only.classes_)\n","df_all_classes_roc_values.info()"]},{"cell_type":"markdown","metadata":{},"source":["Let's isolate the optimal values per class/category/news genre."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_all_classes_optimal_roc_values = df_all_classes_roc_values.drop_duplicates(subset=['Class']).reset_index()[['Class','Threshold_Optimal','GMean_Optimal','FPR_Optimal','TPR_Optimal']]\n","\n","df_all_classes_optimal_roc_values"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["type(Y_probability_a_td_only)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def plot_class_roc_curve(class_of_interest, Y_one_vs_all, Y_prob_a, df_class_row):\n","    '''\n","    ## Purpose\n","    `plot_class_roc_curve`: Automate the visualizing of the FPR vs TPR of a particular class/category using the `RocCurveDisplay` function\n","\n","    ## Parameters\n","    - `class_of_interest` (String): Category/class name to isolate\n","    - `Y_one_vs_all` (numpy array): List (array) of binarized values produced by `LabelBinarizer`'s `.transform()` on the Y_test data set.\n","        - NOTE: Function assumes that `label_binarizer` as an object with that name has been initialized and assigned as such.\n","    - `Y_prob_a` (numpy array): Array of probability estimates for each class/category.\n","    - `df_class_row` (row from pandas Dataframe): This dataframe row includes the optimal\n","\n","    ## Return Values\n","    - None. Instead, it \"shows\" the matplotlib plot object.\n","    '''\n","    class_id = np.flatnonzero(label_binarizer.classes_ == class_of_interest)[0]\n","\n","    RocCurveDisplay.from_predictions(\n","        Y_one_vs_all[:, class_id],\n","        Y_prob_a[:, class_id],\n","        name=f\"{class_of_interest} vs the rest\",\n","        color=\"white\",\n","        plot_chance_level=True,\n","    )\n","\n","    # Plot best threshold\n","    # x = FPR_Optimal, y = TPR_Optimal\n","    plt.plot(\n","        df_class_row.FPR_Optimal, \n","        df_class_row.TPR_Optimal,\n","        marker=\"o\",\n","        markerfacecolor='white',\n","        markeredgecolor='black',\n","        markersize=10,\n","    )\n","    opt_x = df_class_row.FPR_Optimal\n","    opt_y = df_class_row.TPR_Optimal\n","    plt.annotate(\n","        f\"Optimal threshold for {class_of_interest} ({str(opt_y.values.tolist()[0])})\",\n","        (opt_x, opt_y), #x,y point to label\n","        xytext=(opt_x+0.03, opt_y-0.05)\n","    )\n","\n","    plt.style.use('cyberpunk')\n","    plt.xlabel(\"False Positive Rate\")\n","    plt.ylabel(\"True Positive Rate\")\n","    plt.title(f\"One-vs-Rest ROC curves:\\n{class_of_interest} vs Rest\", color='white')\n","    plt.tight_layout()\n","    mplcyberpunk.add_glow_effects()\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class_of_interest = \"GREEN\"\n","plot_class_roc_curve(\n","  class_of_interest=class_of_interest,\n","  Y_one_vs_all=y_onehot_test,\n","  Y_prob_a=Y_probability_a_td_only,\n","  df_class_row=df_all_classes_optimal_roc_values.loc[df_all_classes_optimal_roc_values.Class == class_of_interest]\n",")"]},{"cell_type":"markdown","metadata":{},"source":["#### 4.4.5 `text_desc` only & count-based modeling assessment\n","\n","- **Accuracy**: \n","- **MRR**:\n","- **Insert Observation #1**\n","  - Enter thoughts about this observation based on the available assessment evidence above\n","- **Insert Observation #2**\n","  - Enter thoughts about this observation based on the available assessment evidence above\n","- ..."]},{"cell_type":"markdown","metadata":{"id":"MxURZ9z4lgtI"},"source":["### 4.5 Model 2 - `tfidf` features with `text_desc_headline` - short description + headline"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":77839,"status":"ok","timestamp":1649709433418,"user":{"displayName":"Chris Lindgren","userId":"04896671906605823933"},"user_tz":240},"id":"0UCVmsJplgtI","outputId":"5bb3d97a-f5c1-4998-827e-096ec6155215"},"outputs":[],"source":["field='text_desc_headline'\n","feature_rep='tfidf'\n","top_k=3\n","\n","model_tfidf_tdh,transformer_tfidf_tdh,accuracy_tfidf_tdh,mrr_at_k_tfidf_tdh,X_tfidf_tdh_train,X_tfidf_tdh_test,Y_tfidf_tdh_test,Y_tfidf_tdh_train,preds_tfidf_tdh,eval_items_tfidf_tdh = train_model(\n","  df,\n","  field=field,\n","  feature_rep=feature_rep,\n","  top_k=top_k\n",")\n","\n","print(f\"\\n\\nSimple Mean Average Model Accuracy = {accuracy_tfidf_tdh}\\nMean Reciprocal Rank = {mrr_at_k_tfidf_tdh}\\n\")"]},{"cell_type":"markdown","metadata":{},"source":["#### 4.5.5 `text_desc`, `headline`, & TF-IDF-based modeling assessment\n","\n","Now, use the same assessment techniques as in the `text_desc` only model to assess this model that uses TF-IDF + a combination of the short description and headline values as features for the model.\n","\n","Write your observations and evidence below.\n","\n","- **Accuracy**: \n","- **MRR**:\n","- **Insert Observation #1**\n","  - Enter thoughts about this observation based on the available assessment evidence above\n","- **Insert Observation #2**\n","  - Enter thoughts about this observation based on the available assessment evidence above\n","- ..."]},{"cell_type":"markdown","metadata":{"id":"yq8ySg7jlgtJ"},"source":["### 4.6 Model 3 - `tfidf` features with `text_desc_headline_url`: description, headline, *and* url"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":121811,"status":"ok","timestamp":1649709555223,"user":{"displayName":"Chris Lindgren","userId":"04896671906605823933"},"user_tz":240},"id":"HIshxNadlgtJ","outputId":"38df0398-521e-45ea-f9c8-e2ba27459290"},"outputs":[],"source":["field='text_desc_headline_url'\n","feature_rep='tfidf'\n","top_k=3\n","\n","model_tfidf_all,transformer_tfidf_all,accuracy_tfidf_all,mrr_at_k_tfidf_all,X_tfidf_all_train,X_tfidf_all_test,Y_tfidf_all_test,Y_tfidf_all_train,preds_tfidf_all,eval_items_tfidf_all = train_model(\n","  df,\n","  field=field,\n","  feature_rep=feature_rep,\n","  top_k=top_k\n",")\n","\n","print(\"\\nAccuracy={0}; MRR={1}\".format(accuracy_tfidf_all,mrr_at_k_tfidf_all))"]},{"cell_type":"markdown","metadata":{},"source":["#### 4.6.5 `text_desc`, `headline`, `URL`, & TF-IDF-based modeling assessment\n","\n","Now, use the same assessment techniques as the previous models to assess this third model that uses TF-IDF + a combination of the short description, headline, and URL values as features for the model.\n","\n","Write your observations and evidence below.\n","\n","- **Accuracy**: \n","- **MRR**:\n","- **Insert Observation #1**\n","  - Enter thoughts about this observation based on the available assessment evidence above\n","- **Insert Observation #2**\n","  - Enter thoughts about this observation based on the available assessment evidence above\n","- ..."]},{"cell_type":"markdown","metadata":{"id":"62uJ-ivklgtJ"},"source":["## 5. Check Predictions on Unseen Articles"]},{"cell_type":"markdown","metadata":{},"source":["Try from articles that aren't from HuffPost: CNN, Fox, MSNBC, etc.\n","\n","Remember that the algorithm will return the top `k` probabilities. I've created a default of 3 for `k`. The first value in the list is the top returned predicted value."]},{"cell_type":"markdown","metadata":{},"source":["### 5.1 Model 1 trained by counts method &amp; the features from the short description only"]},{"cell_type":"markdown","metadata":{},"source":["This uses Model 1: \n","\n","- Model: `model_td_only`\n","- Transformer: `transformer_td_only`"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1649709555225,"user":{"displayName":"Chris Lindgren","userId":"04896671906605823933"},"user_tz":240},"id":"kBRHmMALlgtJ","outputId":"63d93f5c-0214-429c-cb27-e1d98d7f987b"},"outputs":[],"source":["# https://www.cnn.com/2019/07/19/health/astronaut-exercise-iv-faint-scn/index.html\n","test_features = transformer_td_only.transform([\"Exercise in space keeps astronauts from fainting when they return to Earth, study says.\"])\n","get_top_k_predictions(\n","  model_td_only, \n","  test_features,\n","  3\n",")"]},{"cell_type":"markdown","metadata":{},"source":["### 5.2 Model 2 trained by TF-IDF &amp; the features from the short description &amp; headline"]},{"cell_type":"markdown","metadata":{},"source":["This uses Model 2:\n","\n","- Model: `model_tfidf_tdh`\n","- Transformer: `transformer_tfidf_tdh`"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1649709555224,"user":{"displayName":"Chris Lindgren","userId":"04896671906605823933"},"user_tz":240},"id":"ivvAe28plgtJ","outputId":"4c455efa-fbcd-4d6c-d7d7-18b25cc09592"},"outputs":[],"source":["# URL: https://www.network.com/enter/url/to/story/here.html\n","\n","test_features=transformer_tfidf_tdh.transform(\n","    [\n","     \"Enter headling here from story above\"\n","    ]\n",")\n","\n","get_top_k_predictions(\n","  model_tfidf_tdh,\n","  test_features,\n","  3\n",")"]},{"cell_type":"markdown","metadata":{},"source":["### 5.3 Model 3 trained by TF-IDF &amp; the features from the short description, headline &amp; URL"]},{"cell_type":"markdown","metadata":{},"source":["This uses Model 3:\n","\n","- Model: `model_tfidf_all`\n","- Transformer: `transformer_tfidf_all`"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# URL: https://www.network.com/enter/url/to/story/here.html\n","test_features=transformer_tfidf_tdh.transform(\n","    [\n","     \"Enter headling here from story above\"\n","    ]\n",")\n","\n","get_top_k_predictions(\n","  model_tfidf_tdh,\n","  test_features,\n","  3\n",")"]},{"cell_type":"markdown","metadata":{"id":"In6wjZxelgtK"},"source":["## 6. Save Models for Future Use\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":287,"status":"aborted","timestamp":1649711042976,"user":{"displayName":"Chris Lindgren","userId":"04896671906605823933"},"user_tz":240},"id":"aF89HEoXlgtK"},"outputs":[],"source":["# Use imported `pickle` code library\n","\n","#### 1. COUNTS - 'text_desc' as features only ####\n","model_path_td_only = \"./../models/news_genre_model_binary_text_desc_only.pkl\"\n","transformer_path_td_only = \"./../models/news_genre_transformer_binary_text_desc_only.pkl\"\n","# Save both the transformer -> to encode a document and the model itself to make predictions based on the weight vectors \n","pickle.dump( model_td_only, open(model_path_td_only, 'wb') )\n","pickle.dump( transformer_td_only, open(transformer_path_td_only,'wb') )\n","\n","#### 2. TF-IDF - 'text_desc_headline' as features ####\n","model_path_tdh = \"./../models/news_genre_model_tfidf_tdh.pkl\"\n","transformer_path_tdh = \"./../models/news_genre_transformer_tfidf_tdh.pkl\"\n","# Save both the transformer -> to encode a document and the model itself to make predictions based on the weight vectors \n","pickle.dump( model_tfidf_tdh, open(model_path_tdh, 'wb') )\n","pickle.dump( transformer_tfidf_tdh, open(transformer_path_tdh,'wb') )\n","\n","#### 3. TF-IDF - 'text_desc_headline_url' as features ####\n","model_path_all = \"./../models/news_genre_model_tfidf_all.pkl\"\n","transformer_path_all = \"./../models/news_genre_transformer_tfidf_all.pkl\"\n","# Save both the transformer -> to encode a document and the model itself to make predictions based on the weight vectors \n","pickle.dump( model_tfidf_all, open(model_path_all, 'wb') )\n","pickle.dump( transformer_tfidf_all, open(transformer_path_all,'wb') )\n"]},{"cell_type":"markdown","metadata":{"id":"J9HN07JXlgtK"},"source":["## 7. Use Loaded Model"]},{"cell_type":"markdown","metadata":{},"source":["With pickle, we can now also load the model and use it without needing to retrain everything. A good time saver to consider for your team's final project!"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":288,"status":"aborted","timestamp":1649711042977,"user":{"displayName":"Chris Lindgren","userId":"04896671906605823933"},"user_tz":240},"id":"VXBz2xQ2lgtK"},"outputs":[],"source":["# Load Model 2 for reference\n","# Note my variable assignments include \"loaded_\" so I can distinguish variable assignments, if necessary.\n","\n","model_path_tdh = \"./../models/news_genre_model_tfidf_tdh.pkl\"\n","transformer_path_tdh = \"./../models/news_genre_transformer_tfidf_tdh.pkl\"\n","\n","loaded_model_tfidf_tdh = pickle.load(open(model_path_tdh, 'rb'))\n","loaded_transformer_tfidf_tdh = pickle.load(open(transformer_path_tdh, 'rb'))\n","\n","# URL: https://www.network.com/enter/url/to/story/here.html\n","loaded_test_features_tfidf_tdh = loaded_transformer_tfidf_tdh.transform([\"Enter headling here from story above\"])\n","get_top_k_predictions(loaded_model_tfidf_tdh, loaded_test_features_tfidf_tdh, 3)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Conclusion"]},{"cell_type":"markdown","metadata":{},"source":["Overall, do your best to have understood this LR modeling/training process: the goals for the model in relationship to the original data set used. By documenting your observations and using Python to conduct EDA + model assessments, you can complete the following work that you will need to repeat for your final project:\n","\n","1. Identify potential boundaries, biases, and limits of the data in relationship to your developing and changing goals for modeling\n","2. Conversely, identify potential possibilities and affordances of the data in relationship to your developing and changing goals for modeling\n","3. Documenting these biases, affrodances, and changes in goals\n","4. Use this notebook material to develop a Model Card that communicates the aforementioned material in a more concise and helpful manner"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Text Classification with Logistic Regression.ipynb","provenance":[]},"interpreter":{"hash":"ca9752e44562eac9eb8af20ed27ecdaee8307d29d279d171455889baf1c568d6"},"kernelspec":{"display_name":"3.8.3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.8"}},"nbformat":4,"nbformat_minor":0}
